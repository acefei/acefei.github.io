<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Acefei 's One Piece</title><link href="https://acefei.github.io/" rel="alternate"></link><link href="https://acefei.github.io/feeds/all.atom.xml" rel="self"></link><id>https://acefei.github.io/</id><updated>2018-01-24T15:24:00+08:00</updated><entry><title>使用cookiecutter+pypackage-minimal快速制作一个python package</title><link href="https://acefei.github.io/shi-yong-cookiecutterpypackage-minimalkuai-su-zhi-zuo-yi-ge-python-package.html" rel="alternate"></link><published>2018-01-24T15:24:00+08:00</published><updated>2018-01-24T15:24:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2018-01-24:/shi-yong-cookiecutterpypackage-minimalkuai-su-zhi-zuo-yi-ge-python-package.html</id><summary type="html">
&lt;h3 id="prerequisites"&gt;Prerequisites&lt;a class="headerlink" href="#prerequisites" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install scrapy
pip install cookiecutter
https://github.com/kragniz/cookiecutter-pypackage-minimal.git
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="start-cookiecutter"&gt;Start &lt;a href="https://github.com/audreyr/cookiecutter"&gt;cookiecutter&lt;/a&gt;&lt;a class="headerlink" href="#start-cookiecutter" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;cookiecutter&lt;/span&gt; &lt;span class="n"&gt;cookiecutter&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pypackage&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;minimal&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;python2&lt;/span&gt;&lt;span class="mf"&gt;.7&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;site&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;py&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;RequestsDependencyWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;urllib3&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.22&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;or&lt;/span&gt; &lt;span class="n"&gt;chardet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.2.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;doesn&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;match&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;supported&lt;/span&gt; &lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
  &lt;span class="n"&gt;RequestsDependencyWarning&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;author_name&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Louis&lt;/span&gt; &lt;span class="n"&gt;Taylor&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;acefei&lt;/span&gt;
&lt;span class="n"&gt;author_email …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">
&lt;h3 id="prerequisites"&gt;Prerequisites&lt;a class="headerlink" href="#prerequisites" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install scrapy
pip install cookiecutter
https://github.com/kragniz/cookiecutter-pypackage-minimal.git
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="start-cookiecutter"&gt;Start &lt;a href="https://github.com/audreyr/cookiecutter"&gt;cookiecutter&lt;/a&gt;&lt;a class="headerlink" href="#start-cookiecutter" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;cookiecutter&lt;/span&gt; &lt;span class="n"&gt;cookiecutter&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pypackage&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;minimal&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;
&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;usr&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;lib&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;python2&lt;/span&gt;&lt;span class="mf"&gt;.7&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;site&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nl"&gt;py&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nl"&gt;RequestsDependencyWarning&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;urllib3&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.22&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;or&lt;/span&gt; &lt;span class="n"&gt;chardet&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;2.2.1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;doesn&lt;/span&gt;&lt;span class="err"&gt;'&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="n"&gt;match&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;supported&lt;/span&gt; &lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;
  &lt;span class="n"&gt;RequestsDependencyWarning&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;author_name&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Louis&lt;/span&gt; &lt;span class="n"&gt;Taylor&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;acefei&lt;/span&gt;
&lt;span class="n"&gt;author_email&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;louis&lt;/span&gt;&lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;kragniz&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eu&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;acefei&lt;/span&gt;&lt;span class="mf"&gt;@163.&lt;/span&gt;&lt;span class="n"&gt;com&lt;/span&gt;
&lt;span class="n"&gt;package_name&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;cookiecutter_pypackage_minimal&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;scrapy_templates&lt;/span&gt;
&lt;span class="n"&gt;package_version&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.1.0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;package_description&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;An&lt;/span&gt; &lt;span class="n"&gt;opinionated&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;minimal&lt;/span&gt; &lt;span class="n"&gt;cookiecutter&lt;/span&gt; &lt;span class="n"&gt;template&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Python&lt;/span&gt; &lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Templates&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;creating&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;projects&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;startproject&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;spiders&lt;/span&gt; &lt;span class="n"&gt;with&lt;/span&gt; &lt;span class="n"&gt;genspider&lt;/span&gt; &lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="n"&gt;package_url&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nl"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//github.com/borntyping/cookiecutter-pypackage-minimal]: https://github.com/acefei/scrapy_templates&lt;/span&gt;
&lt;span class="n"&gt;readme_pypi_badge&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="n"&gt;readme_travis_badge&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;False&lt;/span&gt;
&lt;span class="n"&gt;readme_travis_url&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nl"&gt;https&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="c1"&gt;//travis-ci.org/borntyping/cookiecutter-pypackage-minimal]: False&lt;/span&gt;


&lt;span class="err"&gt;$&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt; &lt;span class="n"&gt;scrapy_templates&lt;/span&gt;&lt;span class="cm"&gt;/*&lt;/span&gt;
&lt;span class="cm"&gt;scrapy_templates/README.rst  scrapy_templates/setup.py  scrapy_templates/tox.ini&lt;/span&gt;

&lt;span class="cm"&gt;scrapy_templates/scrapy_templates:&lt;/span&gt;
&lt;span class="cm"&gt;__init__.py&lt;/span&gt;

&lt;span class="cm"&gt;scrapy_templates/tests:&lt;/span&gt;
&lt;span class="cm"&gt;__init__.py  test_sample.py&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="create-scrapy-template"&gt;Create scrapy template&lt;a class="headerlink" href="#create-scrapy-template" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Copy the original template from scrapy project.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cd scrapy_templates/scrapy_templates/
mkdir scrapy &amp;amp;&amp;amp; cd scrapy
cp -r /usr/lib64/python2.7/site-packages/scrapy/templates/project .
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, you can customize the template in &lt;code&gt;scrapy_templates/scrapy_templates/scrapy&lt;/code&gt;&lt;/p&gt;
&lt;h3 id="add-non-package-files"&gt;Add non-package files&lt;a class="headerlink" href="#add-non-package-files" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Since the templates path is not a python package, to specify those files to distribute, we should add  &lt;a href="https://docs.python.org/2/distutils/sourcedist.html#manifest"&gt;MANIFEST&lt;/a&gt; file.       &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# in MANIFEST
recursive-include  scrapy_templates/  *
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In order for these files to be copied at install time to the package’s folder inside site-packages, you’ll need to supply &lt;code&gt;include_package_data=True&lt;/code&gt; to the setup() function.&lt;/p&gt;
&lt;h3 id="command-line-script"&gt;Command Line Script&lt;a class="headerlink" href="#command-line-script" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;write a python script command_line.py with some functions in the package.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;register those functihons to setup() function.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;setuptools&lt;/span&gt;

&lt;span class="n"&gt;setuptools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;entry_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;'console_scripts'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s1"&gt;'scrapy-startproject=scrapy_templates.command_line:startproj'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="s1"&gt;'scrapy-genspider=scrapy_templates.command_line:genspider'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="specifying-dependencies"&gt;Specifying Dependencies&lt;a class="headerlink" href="#specifying-dependencies" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Edit &lt;code&gt;setup.py&lt;/code&gt;, we just add an install_requires keyword argument:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;setuptools&lt;/span&gt;

&lt;span class="n"&gt;setuptools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="n"&gt;install_requires&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s1"&gt;'scrapy-redis'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;'scrapy-splash'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;'scrapy-random-useragent'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;'scrapy-redis-bloomfilter'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="summary"&gt;Summary&lt;a class="headerlink" href="#summary" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Now, we have finished an python package.&lt;br/&gt;
It's easy to install this package by pip.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install git+https://github.com/acefei/scrapy_templates.git
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;About this package details, please find &lt;a href="https://github.com/acefei/scrapy_templates.git"&gt;acefei/scrapy_templates
&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="inspiration"&gt;Inspiration&lt;a class="headerlink" href="#inspiration" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://python-packaging.readthedocs.io/en/latest/index.html"&gt;python-packaging&lt;/a&gt;&lt;/p&gt;</content><category term="python"></category><category term="cookiecutter"></category><category term="package"></category></entry><entry><title>关于Selenium+PhantomJS设置HTTP PROXY的问题</title><link href="https://acefei.github.io/guan-yu-seleniumphantomjsshe-zhi-http-proxyde-wen-ti.html" rel="alternate"></link><published>2017-12-21T15:12:00+08:00</published><updated>2017-12-21T15:12:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-12-21:/guan-yu-seleniumphantomjsshe-zhi-http-proxyde-wen-ti.html</id><summary type="html">
&lt;p&gt;在写爬虫的时候，经常会遇到anti-spider，这时候我们可以采取切换代理ip来绕过限制。但是最近在Selenium+PhantomJS实践过程中遇到一个很trick的问题，在此做一下记录。&lt;/p&gt;
&lt;h2 id="python-requests-with-proxy"&gt;python requests with proxy&lt;a class="headerlink" href="#python-requests-with-proxy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;一开始我们还是用pythonic的方式来看看，http proxy是如何隐藏真实ip的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"--No Proxy--"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"http://httpbin.org/ip"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;

&lt;span class="c1"&gt;# 在网上随便找个免费http代理&lt;/span&gt;
&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'182.121.201.9:9999'&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"--Proxy {0}--"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;proxies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;"http"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"http://{0}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"http://httpbin.org …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">
&lt;p&gt;在写爬虫的时候，经常会遇到anti-spider，这时候我们可以采取切换代理ip来绕过限制。但是最近在Selenium+PhantomJS实践过程中遇到一个很trick的问题，在此做一下记录。&lt;/p&gt;
&lt;h2 id="python-requests-with-proxy"&gt;python requests with proxy&lt;a class="headerlink" href="#python-requests-with-proxy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;一开始我们还是用pythonic的方式来看看，http proxy是如何隐藏真实ip的。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"--No Proxy--"&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"http://httpbin.org/ip"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;

&lt;span class="c1"&gt;# 在网上随便找个免费http代理&lt;/span&gt;
&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;'182.121.201.9:9999'&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"--Proxy {0}--"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;proxies&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;"http"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"http://{0}"&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"http://httpbin.org/ip"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proxies&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;proxies&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;--No Proxy--
{
  "origin": "36.152.29.163"
}

--Proxy 182.121.201.9:9999--
{
  "origin": "36.152.29.163, 182.121.204.0"
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;可以看出来origin的值插入了一条新的ip，证明设置proxy生效了。&lt;/p&gt;
&lt;p&gt;但是我们要注意，上面使用的代理并不是高匿的，所以从origin里面还是能找到机器的真实ip(仍然会被反爬)。
高匿ip的输出应该是以下结果：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;--No Proxy--
{
  "origin": "36.152.29.163"
}

--Proxy 113.218.218.86:808--
{
  "origin": "113.218.218.86"
}
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="phantomjs-with-proxy"&gt;phantomjs with proxy&lt;a class="headerlink" href="#phantomjs-with-proxy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;编辑httpbin_test.js，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;"use strict";
var page = require('webpage').create();
page.open('http://httpbin.org/ip', function (status) {
    if (status !== 'success') {
        console.log('Unable to access network');
    } else {
        console.log(page.content);
    }
    phantom.exit();
});
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;执行phantomjs命令，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ phantomjs --proxy=182.121.201.9:9999 httpbin_test.js
&lt;span class="nt"&gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;pre&lt;/span&gt; &lt;span class="na"&gt;style=&lt;/span&gt;&lt;span class="s"&gt;"word-wrap: break-word; white-space: pre-wrap;"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;{
  "origin": "36.152.29.163, 182.121.204.0"
}
&lt;span class="nt"&gt;&amp;lt;/pre&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ok，proxy设置成功。&lt;/p&gt;
&lt;h2 id="selenium-phantomjs-with-proxy"&gt;selenium + phantomjs with proxy&lt;a class="headerlink" href="#selenium-phantomjs-with-proxy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Stackoverflow上有人推荐方案，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium.webdriver&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DesiredCapabilities&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium.webdriver.common.proxy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Proxy&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium.webdriver.common.proxy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ProxyType&lt;/span&gt;
&lt;span class="n"&gt;proxy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Proxy&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;'proxyType'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ProxyType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MANUAL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;'httpProxy'&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;'182.121.201.9:9999'&lt;/span&gt;  &lt;span class="c1"&gt;# 代理ip和端口&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="n"&gt;desired_capabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DesiredCapabilities&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PHANTOMJS&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_to_capabilities&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desired_capabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;"add proxy"&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;http_proxy&lt;/span&gt;

&lt;span class="n"&gt;driver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PhantomJS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desired_capabilities&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;desired_capabilities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'http://httpbin.org/ip'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;page_source&lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;output：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;add proxy182.121.201.9:9999
&lt;span class="nt"&gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;pre&lt;/span&gt; &lt;span class="na"&gt;style=&lt;/span&gt;&lt;span class="s"&gt;"word-wrap: break-word; white-space: pre-wrap;"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;{
  "origin": "36.152.29.163"
}
&lt;span class="nt"&gt;&amp;lt;/pre&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;奇怪，这里发现proxy并没有生效，不知道是不是我的phantomjs 1.9.7版本的bug。&lt;/p&gt;
&lt;p&gt;那换个方案试试，&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;selenium&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;

&lt;span class="n"&gt;service_args&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;'--proxy=182.121.201.9:9999'&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;webdriver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PhantomJS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;service_args&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;service_args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;'http://httpbin.org/ip'&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;page_source&lt;/span&gt;
&lt;span class="n"&gt;driver&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;output：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;pre&lt;/span&gt; &lt;span class="na"&gt;style=&lt;/span&gt;&lt;span class="s"&gt;"word-wrap: break-word; white-space: pre-wrap;"&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;{
  "origin": "36.152.29.163, 182.121.204.0"
}
&lt;span class="nt"&gt;&amp;lt;/pre&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ok, proxy设置成功了。&lt;/p&gt;
&lt;p&gt;可以看出来，通过service_args传递参数其实跟phantomjs命令行传参的是一样效果。&lt;/p&gt;
&lt;p&gt;那么，如果想使用带auth的代理，只要加上对应的option就好了，很方便。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$phantomjs -h
Usage:
   phantomjs [switchs] [options] [script] [argument [argument [...]]]

Options:
  --cookies-file=&amp;lt;val&amp;gt;                 Sets the file name to store the persistent cookies
  --config=&amp;lt;val&amp;gt;                       Specifies JSON-formatted configuration file
  --debug=&amp;lt;val&amp;gt;                        Prints additional warning and debug message: 'true' or 'false' (default)
  --disk-cache=&amp;lt;val&amp;gt;                   Enables disk cache: 'true' or 'false' (default)
  --ignore-ssl-errors=&amp;lt;val&amp;gt;            Ignores SSL errors (expired/self-signed certificate errors): 'true' or 'false' (default)
  --load-images=&amp;lt;val&amp;gt;                  Loads all inlined images: 'true' (default) or 'false'
  --local-storage-path=&amp;lt;val&amp;gt;           Specifies the location for offline local storage
  --local-storage-quota=&amp;lt;val&amp;gt;          Sets the maximum size of the offline local storage (in KB)
  --local-to-remote-url-access=&amp;lt;val&amp;gt;   Allows local content to access remote URL: 'true' or 'false' (default)
  --max-disk-cache-size=&amp;lt;val&amp;gt;          Limits the size of the disk cache (in KB)
  --output-encoding=&amp;lt;val&amp;gt;              Sets the encoding for the terminal output, default is 'utf8'
  --remote-debugger-port=&amp;lt;val&amp;gt;         Starts the script in a debug harness and listens on the specified port
  --remote-debugger-autorun=&amp;lt;val&amp;gt;      Runs the script in the debugger immediately: 'true' or 'false' (default)
  --proxy=&amp;lt;val&amp;gt;                        Sets the proxy server, e.g. '--proxy=http://proxy.company.com:8080'
  --proxy-auth=&amp;lt;val&amp;gt;                   Provides authentication information for the proxy, e.g. ''-proxy-auth=username:password'
  --proxy-type=&amp;lt;val&amp;gt;                   Specifies the proxy type, 'http' (default), 'none' (disable completely), or 'socks5'
  --script-encoding=&amp;lt;val&amp;gt;              Sets the encoding used for the starting script, default is 'utf8'
  --web-security=&amp;lt;val&amp;gt;                 Enables web security, 'true' (default) or 'false'
  --ssl-protocol=&amp;lt;val&amp;gt;                 Sets the SSL protocol (supported protocols: 'SSLv3' (default), 'SSLv2', 'TLSv1', 'any')
  --ssl-certificates-path=&amp;lt;val&amp;gt;        Sets the location for custom CA certificates (if none set, uses system default)
  --webdriver=&amp;lt;val&amp;gt;                    Starts in 'Remote WebDriver mode' (embedded GhostDriver): '[[&amp;lt;IP&amp;gt;:]&amp;lt;PORT&amp;gt;]' (default '127.0.0.1:8910')
  --webdriver-logfile=&amp;lt;val&amp;gt;            File where to write the WebDriver's Log (default 'none') (NOTE: needs '--webdriver')
  --webdriver-loglevel=&amp;lt;val&amp;gt;           WebDriver Logging Level: (supported: 'ERROR', 'WARN', 'INFO', 'DEBUG') (default 'INFO') (NOTE: needs '--webdriver')
  --webdriver-selenium-grid-hub=&amp;lt;val&amp;gt;  URL to the Selenium Grid HUB: 'URL_TO_HUB' (default 'none') (NOTE: needs '--webdriver')
  -w,--wd                              Equivalent to '--webdriver' option above
  -h,--help                            Shows this message and quits
  -v,--version                         Prints out PhantomJS version

Any of the options that accept boolean values ('true'/'false') can also accept 'yes'/'no'.

Without any argument, PhantomJS will launch in interactive mode (REPL).

Documentation can be found at the web site, http://phantomjs.org.
&lt;/pre&gt;&lt;/div&gt;</content><category term="python"></category><category term="phantomjs"></category><category term="spider"></category></entry><entry><title>机器学习资料整理(Python方向，建议通读)</title><link href="https://acefei.github.io/ji-qi-xue-xi-zi-liao-zheng-li-pythonfang-xiang-jian-yi-tong-du.html" rel="alternate"></link><published>2017-11-30T09:17:00+08:00</published><updated>2017-11-30T09:17:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-11-30:/ji-qi-xue-xi-zi-liao-zheng-li-pythonfang-xiang-jian-yi-tong-du.html</id><summary type="html">
&lt;p&gt;一些好东西，入门前未必看得懂，要等学有小成时再看才能体会。&lt;/p&gt;
&lt;h1 id="python"&gt;Python&lt;a class="headerlink" href="#python" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://python.jobbole.com/87522/"&gt;致Python初学者：Anaconda入门使用指南&lt;/a&gt;  ——选择anaconda，体验拎包入住的感觉&lt;/p&gt;
&lt;h1 id="_1"&gt;数据处理&lt;a class="headerlink" href="#_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="numpy"&gt;&lt;a href="http://www.numpy.org/"&gt;numpy&lt;/a&gt;&lt;a class="headerlink" href="#numpy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Numpy--Numerical Python，一个基于Python的可以存储和处理大型矩阵的库。几乎是Python 生态系统的数值计算的基石，例如Scipy，Pandas，Scikit-learn，Keras等都基于Numpy。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.scipy.org/doc/numpy-dev/genindex.html"&gt;Index&lt;/a&gt; 查询api，推荐！&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="scipy"&gt;&lt;a href=""&gt;Scipy&lt;/a&gt;&lt;a class="headerlink" href="#scipy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;SciPy函数库在NumPy库的基础上增加了众多的数学、科学以及工程计算中常用的库函数。例如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵等等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.scipy-lectures.org/#"&gt;Scipy Lecture Notes&lt;/a&gt; 极力推荐的一个学习笔记！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pandas"&gt;&lt;a href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;&lt;a class="headerlink" href="#pandas" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;pandas是基于Numpy构建的含有更高级数据结构和工具的数据分析包。
类似于Numpy的核心是ndarray，pandas也是围绕着Series和DataFrame两个核心数据结构展开的。
Series和DataFrame分别对应于一维的序列和二维的表结构。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://pandas.pydata.org/pandas-docs/stable/10min.html"&gt;10 Minutes to pandas&lt;/a&gt; (&lt;a href="http://www.cnblogs.com/chaosimple/p/4153083.html"&gt;翻译&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://pandas.pydata.org/pandas-docs/stable/genindex.html"&gt;Index&lt;/a&gt; 查询api，推荐！&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="_2"&gt;机器算法&lt;a class="headerlink" href="#_2" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="scikit-learn"&gt;&lt;a href="http://scikit-learn.org/stable/"&gt;scikit-learn …&lt;/a&gt;&lt;/h2&gt;</summary><content type="html">
&lt;p&gt;一些好东西，入门前未必看得懂，要等学有小成时再看才能体会。&lt;/p&gt;
&lt;h1 id="python"&gt;Python&lt;a class="headerlink" href="#python" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://python.jobbole.com/87522/"&gt;致Python初学者：Anaconda入门使用指南&lt;/a&gt;  ——选择anaconda，体验拎包入住的感觉&lt;/p&gt;
&lt;h1 id="_1"&gt;数据处理&lt;a class="headerlink" href="#_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="numpy"&gt;&lt;a href="http://www.numpy.org/"&gt;numpy&lt;/a&gt;&lt;a class="headerlink" href="#numpy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Numpy--Numerical Python，一个基于Python的可以存储和处理大型矩阵的库。几乎是Python 生态系统的数值计算的基石，例如Scipy，Pandas，Scikit-learn，Keras等都基于Numpy。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html"&gt;Quickstart&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.scipy.org/doc/numpy-dev/genindex.html"&gt;Index&lt;/a&gt; 查询api，推荐！&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="scipy"&gt;&lt;a href=""&gt;Scipy&lt;/a&gt;&lt;a class="headerlink" href="#scipy" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;SciPy函数库在NumPy库的基础上增加了众多的数学、科学以及工程计算中常用的库函数。例如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵等等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.scipy-lectures.org/#"&gt;Scipy Lecture Notes&lt;/a&gt; 极力推荐的一个学习笔记！&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pandas"&gt;&lt;a href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;&lt;a class="headerlink" href="#pandas" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;pandas是基于Numpy构建的含有更高级数据结构和工具的数据分析包。
类似于Numpy的核心是ndarray，pandas也是围绕着Series和DataFrame两个核心数据结构展开的。
Series和DataFrame分别对应于一维的序列和二维的表结构。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://pandas.pydata.org/pandas-docs/stable/10min.html"&gt;10 Minutes to pandas&lt;/a&gt; (&lt;a href="http://www.cnblogs.com/chaosimple/p/4153083.html"&gt;翻译&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://pandas.pydata.org/pandas-docs/stable/genindex.html"&gt;Index&lt;/a&gt; 查询api，推荐！&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="_2"&gt;机器算法&lt;a class="headerlink" href="#_2" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="scikit-learn"&gt;&lt;a href="http://scikit-learn.org/stable/"&gt;scikit-learn&lt;/a&gt;&lt;a class="headerlink" href="#scikit-learn" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;h1 id="_3"&gt;数据可视化&lt;a class="headerlink" href="#_3" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;h2 id="seaborn"&gt;&lt;a href="http://seaborn.pydata.org/index.html"&gt;Seaborn&lt;/a&gt;&lt;a class="headerlink" href="#seaborn" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://seaborn.pydata.org/tutorial.html"&gt;Seaborn tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="matplotlib"&gt;&lt;a href="http://matplotlib.org/"&gt;Matplotlib&lt;/a&gt;&lt;a class="headerlink" href="#matplotlib" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.labri.fr/perso/nrougier/teaching/matplotlib/"&gt;Matplotlib tutorial&lt;/a&gt; (&lt;a href="http://reverland.org/python/2012/09/07/matplotlib-tutorial"&gt;翻译&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="pyecharts"&gt;&lt;a href="https://github.com/chenjiandongx/pyecharts"&gt;Pyecharts&lt;/a&gt;&lt;a class="headerlink" href="#pyecharts" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;如果想使用pandas和numpy数据，需要转换一下。参考：&lt;a href="https://zhuanlan.zhihu.com/p/28198363"&gt;增加了对 Pandas 和 Numpy 数据的简单处理&lt;/a&gt;&lt;/p&gt;
&lt;h1 id="_4"&gt;扩展阅读&lt;a class="headerlink" href="#_4" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://www.open-open.com/lib/view/open1452600067698.html##1"&gt;python机器学习入门资料梳理&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://jingyan.baidu.com/season/41121"&gt;python 线性代数&lt;/a&gt; 用numpy来讲解&lt;/p&gt;
&lt;p&gt;&lt;a href="https://jingyan.baidu.com/season/45667"&gt;Python 统计分析&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://jingyan.baidu.com/season/43456"&gt;pandas教程&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://jingyan.baidu.com/season/35662"&gt;numpy函数&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/josephmisiti/awesome-machine-learning#python"&gt;awesome-machine-learning&lt;/a&gt;&lt;/p&gt;</content><category term="python"></category></entry><entry><title>How to stop scrapy-redis spider when it's idle</title><link href="https://acefei.github.io/how-to-stop-scrapy-redis-spider-when-its-idle.html" rel="alternate"></link><published>2017-11-21T06:44:00+08:00</published><updated>2017-11-21T06:44:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-11-21:/how-to-stop-scrapy-redis-spider-when-its-idle.html</id><summary type="html">&lt;p&gt;scrapy-redis是以redis为基础的组件替换了原本scrapy的部分功能，让它可以分布式运作。
但是在使用的时候发现，它一旦待爬队列为空，spider不会自动结束，而是一直在等待redis push新的urls，在log末尾里可以看到如下内容：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2017-11-21 08:15:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在scrapy-redis的&lt;a href="https://github.com/rmax/scrapy-redis"&gt;README&lt;/a&gt;中得知:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Max idle time to prevent the spider from being closed when distributed crawling.
# This only works if …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;scrapy-redis是以redis为基础的组件替换了原本scrapy的部分功能，让它可以分布式运作。
但是在使用的时候发现，它一旦待爬队列为空，spider不会自动结束，而是一直在等待redis push新的urls，在log末尾里可以看到如下内容：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2017-11-21 08:15:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;在scrapy-redis的&lt;a href="https://github.com/rmax/scrapy-redis"&gt;README&lt;/a&gt;中得知:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Max idle time to prevent the spider from being closed when distributed crawling.
# This only works if queue class is SpiderQueue or SpiderStack,
# and may also block the same time when your spider start at the first time (because the queue is empty).
SCHEDULER_IDLE_BEFORE_CLOSE = 10
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;添加到settings后，发现spider还是不会退出，只是不停的报exception。 &lt;/p&gt;
&lt;p&gt;重新想办法，既然log中能知道spider的状态，那我们就再加一个判断，连续出现X次scrapyed 0 itmes就退出不就好了么。&lt;/p&gt;
&lt;p&gt;继续研读源码发现，这段log是scrapy extensions实现的，而且scrapy支持自定义extensions。&lt;/p&gt;
&lt;p&gt;照着logsstats实现一个&lt;a href="https://github.com/acefei/ace-crawler/blob/master/prototypes/extensions.py"&gt;close spider extension&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;并添加如下配置到settings中：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;EXTENSIONS = {
     &amp;#39;prototypes.extensions.CloseSpiderRedis&amp;#39;: 100,
},
CLOSE_SPIDER_AFTER_IDLE_TIMES = 5
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Done！&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2017-11-21 08:15:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-21 08:16:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-21 08:17:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-21 08:18:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-21 08:19:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-11-21 08:19:10 [scrapy.core.engine] INFO: Closing spider (close spider after 5 times of spider idle)
2017-11-21 08:19:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{&amp;#39;downloader/request_bytes&amp;#39;: 493,
 &amp;#39;downloader/request_count&amp;#39;: 2,
 &amp;#39;downloader/request_method_count/GET&amp;#39;: 2,
 &amp;#39;downloader/response_bytes&amp;#39;: 23397,
 &amp;#39;downloader/response_count&amp;#39;: 2,
 &amp;#39;downloader/response_status_count/200&amp;#39;: 1,
 &amp;#39;downloader/response_status_count/302&amp;#39;: 1,
 &amp;#39;finish_reason&amp;#39;: &amp;#39;close spider after 5 times of spider idle&amp;#39;,
 &amp;#39;finish_time&amp;#39;: datetime.datetime(2017, 11, 21, 8, 19, 10, 770725),
 &amp;#39;log_count/DEBUG&amp;#39;: 6,
 &amp;#39;log_count/INFO&amp;#39;: 14,
 &amp;#39;memusage/max&amp;#39;: 936964096,
 &amp;#39;memusage/startup&amp;#39;: 936964096,
 &amp;#39;response_received_count&amp;#39;: 1,
 &amp;#39;scheduler/dequeued/redis&amp;#39;: 2,
 &amp;#39;scheduler/enqueued/redis&amp;#39;: 2,
 &amp;#39;start_time&amp;#39;: datetime.datetime(2017, 11, 21, 8, 13, 10, 735722)}
2017-11-21 08:19:10 [scrapy.core.engine] INFO: Spider closed (close spider after 5 times of spider idle)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;最后要注意，默认scrapy-redis退出后，会清掉requests/dupefilter的内容，如果你想保留配置，请务必在settings中加上：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Don&amp;#39;t cleanup redis queues, allows to pause/resume crawls.
SCHEDULER_PERSIST = True
&lt;/pre&gt;&lt;/div&gt;</content><category term="scrapy"></category><category term="python"></category></entry><entry><title>使用Scrapyd和Scrapyd-client部署爬虫</title><link href="https://acefei.github.io/shi-yong-scrapydhe-scrapyd-clientbu-shu-pa-chong.html" rel="alternate"></link><published>2017-10-30T09:17:00+08:00</published><updated>2017-10-30T09:17:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-10-30:/shi-yong-scrapydhe-scrapyd-clientbu-shu-pa-chong.html</id><summary type="html">
&lt;h3 id="_1"&gt;基本介绍&lt;a class="headerlink" href="#_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="scrapyd"&gt;&lt;a href="https://github.com/scrapy/scrapyd"&gt;Scrapyd&lt;/a&gt;&lt;a class="headerlink" href="#scrapyd" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Scrapyd is a service for running Scrapy spiders.
It allows you to deploy your Scrapy projects and control their spiders using a HTTP JSON API.  &lt;br/&gt;
Scrapyd can manage multiple projects and each project can have multiple versions uploaded, but only the latest one will be used for launching …&lt;/p&gt;</summary><content type="html">
&lt;h3 id="_1"&gt;基本介绍&lt;a class="headerlink" href="#_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="scrapyd"&gt;&lt;a href="https://github.com/scrapy/scrapyd"&gt;Scrapyd&lt;/a&gt;&lt;a class="headerlink" href="#scrapyd" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Scrapyd is a service for running Scrapy spiders.
It allows you to deploy your Scrapy projects and control their spiders using a HTTP JSON API.  &lt;br/&gt;
Scrapyd can manage multiple projects and each project can have multiple versions uploaded, but only the latest one will be used for launching new spiders.&lt;/p&gt;
&lt;h4 id="scrapyd-client"&gt;&lt;a href="https://github.com/scrapy/scrapyd-client"&gt;Scrapyd-client&lt;/a&gt;&lt;a class="headerlink" href="#scrapyd-client" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Scrapyd-client is a client for Scrapyd. It provides the general scrapyd-client and the scrapyd-deploy utility which allows you to deploy your project to a Scrapyd server.&lt;/p&gt;
&lt;h3 id="_2"&gt;安装&lt;a class="headerlink" href="#_2" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install scrapyd scrapyd-client
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="_3"&gt;配置服务器信息&lt;a class="headerlink" href="#_3" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;当我们使用scrapy startnewproject来创建新工程时，会自动生成一个scrapy.cfg文件。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# Automatically created by: scrapy startproject
#
# For more information about the [deploy] section see:
# https://scrapyd.readthedocs.org/en/latest/deploy.html

[settings]
default = prototypes.settings

[deploy:proto_server]
url = http://localhost:6800/
project = prototypes
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中有两个地方是需要我们更新的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;proto_server是自定义的服务器名称       &lt;br/&gt;
url是指scrapyd启动后，默认的服务器地址&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="scrapyd_1"&gt;启动scrapyd服务&lt;a class="headerlink" href="#scrapyd_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;执行scrapyd，然后用浏览器打开http://localhost:6800/查看界面是否启动成功&lt;/p&gt;
&lt;h3 id="project"&gt;部署project&lt;a class="headerlink" href="#project" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;部署有两种方式：
1. 通过json API:
&lt;a href="http://scrapyd.readthedocs.io/en/stable/api.html?highlight=egg#addversion-json"&gt;addversion.json&lt;/a&gt; &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl http://localhost:6800/addversion.json -F project=prototypes -F version=r23 -F egg=@prototypes.egg
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;其中egg需要用scrapyd-deploy来生成&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# 进入scrapy工程目录
scrapyd-deploy --build-egg prototypes.egg
&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;通过scrapyd-deploy一键部署&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scrapyd-deploy proto_server -p prototypes
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;部署好后，我们可以用命令查看&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;scrapyd-deploy -L proto_server
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;然后在http://localhost:6800/查看界面发现Available projects：&lt;strong&gt;prototypes&lt;/strong&gt; &lt;br/&gt;
&lt;img alt="image" src="http://note.youdao.com/favicon.ico"/&gt;&lt;/p&gt;
&lt;h3 id="spider"&gt;运行spider&lt;a class="headerlink" href="#spider" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;curl http://localhost:6800/schedule.json -d project=prototypes -d spider=qichacha
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Note:
请务必确认在spider脚本中的初始化中对父类也进行初始化，否则会报错&lt;code&gt;exceptions.TypeError: __init__() got an unexpected keyword argument '_job'&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id="further"&gt;Further&lt;a class="headerlink" href="#further" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;[ ]  集成&lt;a href="https://github.com/DormyMo/SpiderKeeper"&gt;DormyMo/SpiderKeeper&lt;/a&gt;或&lt;a href="https://github.com/Gerapy/Gerapy"&gt;Gerapy/Gerapy&lt;/a&gt;&lt;/p&gt;</content><category term="python"></category><category term="scrapy"></category></entry><entry><title>在Centos7上使用Python操作Mysql实践</title><link href="https://acefei.github.io/zai-centos7shang-shi-yong-pythoncao-zuo-mysqlshi-jian.html" rel="alternate"></link><published>2017-10-27T01:14:00+08:00</published><updated>2017-10-27T01:14:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-10-27:/zai-centos7shang-shi-yong-pythoncao-zuo-mysqlshi-jian.html</id><summary type="html">
&lt;h1 id="centos7mysql"&gt;Centos7上安装mysql&lt;a class="headerlink" href="#centos7mysql" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo yum install mysql mysql-devel
# centos7上已经使用mariadb替换了mysql，它们仅在安装配置上有些区别，使用上没有差别。
sudo yum install mariadb-server
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="mysql"&gt;配置mysql&lt;a class="headerlink" href="#mysql" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# 第一次启动会检查/var/lib/mysql目录是否为空，不为空将返回失败
sudo service mariadb start
# 设置mysql密码
sudo mysql_secure_installation
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果配置时有任何错误，可在/var/log/mariadb/mariadb.log里查看具体原因&lt;/p&gt;
&lt;h1 id="python-mysql"&gt;Python Mysql库安装&lt;a class="headerlink" href="#python-mysql" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo yum install python-devel
sudo pip install mysql-python
# Toolkit for Python-based database access.
sudo pip …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">
&lt;h1 id="centos7mysql"&gt;Centos7上安装mysql&lt;a class="headerlink" href="#centos7mysql" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo yum install mysql mysql-devel
# centos7上已经使用mariadb替换了mysql，它们仅在安装配置上有些区别，使用上没有差别。
sudo yum install mariadb-server
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="mysql"&gt;配置mysql&lt;a class="headerlink" href="#mysql" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# 第一次启动会检查/var/lib/mysql目录是否为空，不为空将返回失败
sudo service mariadb start
# 设置mysql密码
sudo mysql_secure_installation
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;如果配置时有任何错误，可在/var/log/mariadb/mariadb.log里查看具体原因&lt;/p&gt;
&lt;h1 id="python-mysql"&gt;Python Mysql库安装&lt;a class="headerlink" href="#python-mysql" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo yum install python-devel
sudo pip install mysql-python
# Toolkit for Python-based database access.
sudo pip install dataset
&lt;/pre&gt;&lt;/div&gt;
&lt;h1 id="dataset"&gt;dataset常见操作&lt;a class="headerlink" href="#dataset" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href="http://dataset.readthedocs.io/en/latest/api.html"&gt;dataset&lt;/a&gt;库是对SQLAlchemy二次封装，使其操作数据库就像操作json文件一样方便。&lt;/p&gt;
&lt;h2 id="mysql_1"&gt;连接Mysql&lt;a class="headerlink" href="#mysql_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;db = dataset.connect('mysql://username:password@port/dbname?charset=utf8')
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="_1"&gt;添加记录&lt;a class="headerlink" href="#_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = {
    'key': 1,
    'column2': 2,
    'column3': 3,
}
# 插入记录，如果key，存在则报错
db[t_name].insert(data, ['key'])
# 插入记录，如果key，存在则更新
db[t_name].upsert(data, ['key'])
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="_2"&gt;查找记录&lt;a class="headerlink" href="#_2" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# 打印表的字段
print self.db['t_name'].columns

# 打印表中所有数据
for row_data in self.db['t_name'].all():
    print row_data

# 根据多个条件找记录, 返回的是列表
self.db['t_name'].find(key1 = 1, key2 =2)

# 返回一条记录
self.db['t_name'].find_one(key1 = 1, key2 =2)
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;考虑到find()查找性能，也可以使用原生的sql语句来进行query&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;select * from t_name limit 99
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="_3"&gt;更新记录&lt;a class="headerlink" href="#_3" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data = {
    'key': 1,
    'column2': 2,
    'column3': 3,
}
self.db['t_name'].update(data, ['key'])
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="_4"&gt;删除记录&lt;a class="headerlink" href="#_4" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;self.db['t_name'].delete(key1=1, key2=2)
# 删除所有记录
self.db['t_name'].delete()
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="dataset_1"&gt;关于dataset导出数据&lt;a class="headerlink" href="#dataset_1" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;现在dataset被分离成两个模块，如果想导出数据请看&lt;a href="https://github.com/pudo/datafreeze"&gt;datafreeze&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;详细用法请见&lt;a href="https://dataset.readthedocs.io/en/latest/"&gt;dataset readthedocs&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="dataset"></category></entry><entry><title>如何在unittest cases之间传递变量</title><link href="https://acefei.github.io/ru-he-zai-unittest-caseszhi-jian-chuan-di-bian-liang.html" rel="alternate"></link><published>2017-10-26T09:53:00+08:00</published><updated>2017-10-26T09:53:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-10-26:/ru-he-zai-unittest-caseszhi-jian-chuan-di-bian-liang.html</id><summary type="html">&lt;p&gt;想必大家在写Python UT的时候，偶尔会遇到一些测试用例需要同时测试CRUD的接口。
这时需要在test cases之间传递变量该如何做呢？&lt;/p&gt;
&lt;p&gt;一开始，我们想既然是在一个class里，直接给instance增加attribute不就好了么?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;unittest&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TestA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;unittest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TestCase&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;testname&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TestA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testname&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hello world&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;unittest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbosity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;结果...想当然了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test_1 (__main__ …&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;想必大家在写Python UT的时候，偶尔会遇到一些测试用例需要同时测试CRUD的接口。
这时需要在test cases之间传递变量该如何做呢？&lt;/p&gt;
&lt;p&gt;一开始，我们想既然是在一个class里，直接给instance增加attribute不就好了么?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;unittest&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TestA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;unittest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TestCase&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;testname&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TestA&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testname&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hello world&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;unittest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbosity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;结果...想当然了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test_1 (__main__.TestA) ... ok
test_2 (__main__.TestA) ... None
ok

----------------------------------------------------------------------
Ran 2 tests in 0.000s

OK
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;知识点一：在unittest中, 执行每个testcase的时候，会刷新instance attribute&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;既然self不行，那cls呢？&lt;/p&gt;
&lt;p&gt;那我们再想想unittest在执行class级别的setup，teardown的时候，其中的class attribute是不是对每个testcase都生效。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;通过查看源码可以知道self.&lt;strong&gt;class&lt;/strong&gt; is cls。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;unittest&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;TestA&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;unittest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;TestCase&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__class__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hello world&amp;#39;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__class__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transmit_var&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;unittest&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;verbosity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Bingo！ &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;test_1 (__main__.TestA) ... ok
test_2 (__main__.TestA) ... hello world
ok

----------------------------------------------------------------------
Ran 2 tests in 0.000s

OK
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;知识点二：在unittest中，绑定在class的attribute可以在testcase之间传递&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;温故而知新：
&lt;a href="https://www.python-course.eu/python3_class_and_instance_attributes.php"&gt;Class and Instance Attributes&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="unittest"></category></entry><entry><title>BloomFilter For Scrapy Redis</title><link href="https://acefei.github.io/bloomfilter-for-scrapy-redis.html" rel="alternate"></link><published>2017-09-21T15:29:00+08:00</published><updated>2017-09-21T15:29:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-09-21:/bloomfilter-for-scrapy-redis.html</id><summary type="html">&lt;h3 id="summary"&gt;Summary:&lt;a class="headerlink" href="#summary" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This article will illustrate how to renovate scrapy-redis to dupefilter.&lt;/p&gt;
&lt;h3 id="why-use-bloomfilter"&gt;Why use bloomfilter&lt;a class="headerlink" href="#why-use-bloomfilter" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://alexeyvishnevsky.com/2013/11/tips-on-optimizing-scrapy-for-a-high-performance/"&gt;Tips on optimizing scrapy for a high performance&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="how-to-integrate-bloomfilter-into-scrapy-redis"&gt;How to integrate bloomfilter into scrapy redis&lt;a class="headerlink" href="#how-to-integrate-bloomfilter-into-scrapy-redis" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;copy scrapy_redis into the path alongside settings in scrapy project&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;implement bloom_filter.py in scrapy_redis path
   the code in https://github …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h3 id="summary"&gt;Summary:&lt;a class="headerlink" href="#summary" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;This article will illustrate how to renovate scrapy-redis to dupefilter.&lt;/p&gt;
&lt;h3 id="why-use-bloomfilter"&gt;Why use bloomfilter&lt;a class="headerlink" href="#why-use-bloomfilter" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://alexeyvishnevsky.com/2013/11/tips-on-optimizing-scrapy-for-a-high-performance/"&gt;Tips on optimizing scrapy for a high performance&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="how-to-integrate-bloomfilter-into-scrapy-redis"&gt;How to integrate bloomfilter into scrapy redis&lt;a class="headerlink" href="#how-to-integrate-bloomfilter-into-scrapy-redis" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;copy scrapy_redis into the path alongside settings in scrapy project&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;implement bloom_filter.py in scrapy_redis path
   the code in https://github.com/acefei/scrapy-redis-docker/blob/master/scrapy_redis_demo/scrapy_redis_demo/bloom_filter.py&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;modify scrapy_redis/dupefilter.py&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    def __init__(self, server, key, debug=False):
        &amp;quot;&amp;quot;&amp;quot;Initialize the duplicates filter.

        Parameters
        ----------
        server : redis.StrictRedis
            The redis server instance.
        key : str
            Redis key Where to store fingerprints.
        debug : bool, optional
            Whether to log filtered requests.

        &amp;quot;&amp;quot;&amp;quot;
        self.server = server
        self.key = key
        self.debug = debug
        self.logdupes = True

        # 集成布隆过滤器
        self.bf = BloomFilter(conn=server, key=key)     # 利用连接池连接Redis
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    def request_seen(self, request):
        &amp;quot;&amp;quot;&amp;quot;Returns True if request was already seen.

        Parameters
        ----------
        request : scrapy.http.Request

        Returns
        -------
        bool

        &amp;quot;&amp;quot;&amp;quot;
        # 集成布隆过滤器
        if self.bf.is_exist(fp):    # 判断如果域名在Redis里存在
            return True
        else:
            self.bf.add(fp)         # 如果不存在，将域名添加到Redis
            return False

        #fp = self.request_fingerprint(request)
        # This returns the number of values added, zero if already exists.
        #added = self.server.sadd(self.key, fp)
        #return added == 0
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;Add scrapy_redis configration into settings.py
Note: use our scrapy_redis code like &lt;scrapy_project_name&gt;.scrapy_redis instead of scrapy_redis that created by pip install&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;##################################
# Configuration for scrapy-redis {
DUPEFILTER_CLASS = &amp;quot;scrapy_redis_demo.scrapy_redis.dupefilter.RFPDupeFilter&amp;quot;
SCHEDULER = &amp;quot;scrapy_redis_demo.scrapy_redis.scheduler.Scheduler&amp;quot;
SCHEDULER_PERSIST = True

ITEM_PIPELINES = {
  &amp;#39;scrapy_redis_demo.scrapy_redis.pipelines.RedisPipeline&amp;#39;: 400,
}

# if u add &amp;#39;network_mode: &amp;quot;host&amp;quot;&amp;#39; in scraper service in docker-compose.yaml
# use host ip to access redis server
REDIS_HOST = &amp;#39;172.16.100.62&amp;#39;
# else use redis hostname to access redis server
#REDIS_HOST = &amp;#39;redis&amp;#39;
REDIS_PORT = 6379

# Specify your redis uri
# the uri scheme syntax: http://www.iana.org/assignments/uri-schemes/prov/redis
#REDIS_URL = &amp;#39;redis://172.16.100.62:6379&amp;#39;

# }
##################################
&lt;/pre&gt;&lt;/div&gt;


&lt;ol&gt;
&lt;li&gt;import &lt;scrapy_project_name&gt;.scrapy_redis in your spiders/xxxxx.py&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;scrapy_redis_demo.scrapy_redis.spiders&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RedisCrawlSpider&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="Python"></category><category term="Scrapy"></category></entry><entry><title>Web Scraping Practice</title><link href="https://acefei.github.io/web-scraping-practice.html" rel="alternate"></link><published>2017-06-23T02:37:00+08:00</published><updated>2017-06-23T02:37:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-06-23:/web-scraping-practice.html</id><summary type="html">&lt;h3 id="summary"&gt;Summary&lt;a class="headerlink" href="#summary" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Recently, I found a &lt;a href="http://www.apexlegend.com/eas/media/"&gt;TOEIC test website&lt;/a&gt; with enomous mp3 materials.&lt;/p&gt;
&lt;p&gt;Just in time, I decided to scrape and download MP3 in parallel by python &lt;/p&gt;
&lt;h3 id="requirement"&gt;Requirement&lt;a class="headerlink" href="#requirement" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install lxml
# https://github.com/kennethreitz/grequests
pip install grequests
&lt;/pre&gt;&lt;/div&gt;


&lt;h3 id="html-scraping"&gt;HTML Scraping&lt;a class="headerlink" href="#html-scraping" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://lxml.de/"&gt;lxml&lt;/a&gt; is a pretty extensive library written for parsing XML …&lt;/p&gt;</summary><content type="html">&lt;h3 id="summary"&gt;Summary&lt;a class="headerlink" href="#summary" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Recently, I found a &lt;a href="http://www.apexlegend.com/eas/media/"&gt;TOEIC test website&lt;/a&gt; with enomous mp3 materials.&lt;/p&gt;
&lt;p&gt;Just in time, I decided to scrape and download MP3 in parallel by python &lt;/p&gt;
&lt;h3 id="requirement"&gt;Requirement&lt;a class="headerlink" href="#requirement" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install lxml
# https://github.com/kennethreitz/grequests
pip install grequests
&lt;/pre&gt;&lt;/div&gt;


&lt;h3 id="html-scraping"&gt;HTML Scraping&lt;a class="headerlink" href="#html-scraping" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;a href="http://lxml.de/"&gt;lxml&lt;/a&gt; is a pretty extensive library written for parsing XML and HTML documents very quickly, even handling messed up tags in the process. We will also be using the &lt;a href="http://docs.python-requests.org/en/latest/"&gt;Requests&lt;/a&gt; module instead of the already built-in urllib2 module due to improvements in speed and readability. &lt;/p&gt;
&lt;p&gt;About the details, please refer to &lt;a href="http://docs.python-guide.org/en/latest/scenarios/scrape/#html-scraping"&gt;the link&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="concurrency"&gt;Concurrency&lt;a class="headerlink" href="#concurrency" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We will use &lt;a href="https://github.com/kennethreitz/grequests"&gt;GRequests&lt;/a&gt; which allows you to use Requests with Gevent to make asynchronous HTTP Requests easily. Or use the fork: &lt;a href="https://github.com/i-trofimtschuk/frequests"&gt;FRequests&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;The following need to be noted:&lt;/p&gt;
&lt;h4 id="1-using-pool-to-limit-concurrency"&gt;1. Using pool to limit concurrency.&lt;a class="headerlink" href="#1-using-pool-to-limit-concurrency" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;GRequests doesn't use pool by default, please see below code snippet: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def map(requests, stream=False, size=None, exception_handler=None, gtimeout=None):
    &amp;quot;&amp;quot;&amp;quot;Concurrently converts a list of Requests to Responses.
    :param requests: a collection of Request objects.
    :param stream: If True, the content will not be downloaded immediately.
    :param size: Specifies the number of requests to make at a time. If None, no throttling occurs.
    :param exception_handler: Callback function, called when exception occured. Params: Request, Exception
    :param gtimeout: Gevent joinall timeout in seconds. (Note: unrelated to requests timeout)
    &amp;quot;&amp;quot;&amp;quot;

    requests = list(requests)

    pool = Pool(size) if size else None

...
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In my requirement, there are enomous links need to settle. If don't constrain concurrency, it would lead to the exception "gevent.hub.LoopExit: This operation would block forever".&lt;/p&gt;
&lt;h4 id="2-using-session-to-avoid-the-fd-consuming"&gt;2. Using session to avoid the fd consuming.&lt;a class="headerlink" href="#2-using-session-to-avoid-the-fd-consuming" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;There is a &lt;a href="https://github.com/kennethreitz/grequests/issues/54"&gt;same problem&lt;/a&gt; with me.&lt;/p&gt;
&lt;p&gt;Finally, the source code is available in &lt;a href="https://gist.github.com/acefei/2ca602d67e53011878dbf40f1ccda216#file-fetch_mp3_from_apexlegend-py"&gt;the gist&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="see-also"&gt;SEE ALSO:&lt;a class="headerlink" href="#see-also" title="Permanent link"&gt;&amp;para;&lt;/a&gt;&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;http://xlambda.com/gevent-tutorial/&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="grequests"></category></entry><entry><title>Make a Github Pages blog with Pelican</title><link href="https://acefei.github.io/make-a-github-pages-blog-with-pelican.html" rel="alternate"></link><published>2017-05-25T10:05:00+08:00</published><updated>2017-05-25T10:05:00+08:00</updated><author><name>Ace Fei</name></author><id>tag:acefei.github.io,2017-05-25:/make-a-github-pages-blog-with-pelican.html</id><summary type="html">
&lt;h2 id="make-a-github-pages-blog-with-pelican"&gt;Make a Github Pages blog with Pelican:&lt;a class="headerlink" href="#make-a-github-pages-blog-with-pelican" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="install-dependence"&gt;Install dependence&lt;a class="headerlink" href="#install-dependence" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;yum&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;git&lt;/span&gt;
&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;pelican&lt;/span&gt; &lt;span class="n"&gt;markdown&lt;/span&gt; &lt;span class="n"&gt;ghp&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;BeautifulSoup4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="create-user-pages"&gt;Create user pages&lt;a class="headerlink" href="#create-user-pages" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are two basic types of GitHub Pages: &lt;a href="https://help.github.com/articles/user-organization-and-project-pages/"&gt;User/Organization Pages and Project Pages&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Generally, most people will select User Pages, and there are two …&lt;/p&gt;</summary><content type="html">
&lt;h2 id="make-a-github-pages-blog-with-pelican"&gt;Make a Github Pages blog with Pelican:&lt;a class="headerlink" href="#make-a-github-pages-blog-with-pelican" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;h3 id="install-dependence"&gt;Install dependence&lt;a class="headerlink" href="#install-dependence" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;yum&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="n"&gt;git&lt;/span&gt;
&lt;span class="n"&gt;sudo&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;pelican&lt;/span&gt; &lt;span class="n"&gt;markdown&lt;/span&gt; &lt;span class="n"&gt;ghp&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;BeautifulSoup4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="create-user-pages"&gt;Create user pages&lt;a class="headerlink" href="#create-user-pages" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;There are two basic types of GitHub Pages: &lt;a href="https://help.github.com/articles/user-organization-and-project-pages/"&gt;User/Organization Pages and Project Pages&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Generally, most people will select User Pages, and there are two caveat as below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You must use the &lt;em&gt;username.github.io&lt;/em&gt; naming scheme.
Content from the &lt;em&gt;master&lt;/em&gt; branch will be used to build and publish your GitHub Pages site.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When User Pages are built, they are available at http(s)://username.github.io.&lt;/p&gt;
&lt;h3 id="set-up-the-blog-with-pelican"&gt;Set up the blog with Pelican&lt;a class="headerlink" href="#set-up-the-blog-with-pelican" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Create a new branch (pelican) for hosting pelican settings on github, please refer to &lt;a href="#publish"&gt;Publish&lt;/a&gt; &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ git clone https://github.com/acefei/acefei.github.io
$ &lt;span class="nb"&gt;cd&lt;/span&gt; acefei.github.io/
$ git checkout -b pelican
$ pelican-quickstart
Welcome to pelican-quickstart v3.7.1.

This script will &lt;span class="nb"&gt;help&lt;/span&gt; you create a new Pelican-based website.

Please answer the following questions so this script can generate the files
needed by Pelican.


&amp;gt; Where &lt;span class="k"&gt;do&lt;/span&gt; you want to create your new web site? &lt;span class="o"&gt;[&lt;/span&gt;.&lt;span class="o"&gt;]&lt;/span&gt;
&amp;gt; What will be the title of this web site? Acefei&lt;span class="err"&gt;'&lt;/span&gt;s Blog
&amp;gt; Who will be the author of this web site? acefei
&amp;gt; What will be the default language of this web site? &lt;span class="o"&gt;[&lt;/span&gt;en&lt;span class="o"&gt;]&lt;/span&gt;
&amp;gt; Do you want to specify a URL prefix? e.g., http://example.com   &lt;span class="o"&gt;(&lt;/span&gt;Y/n&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; What is your URL prefix? &lt;span class="o"&gt;(&lt;/span&gt;see above example&lt;span class="p"&gt;;&lt;/span&gt; no trailing slash&lt;span class="o"&gt;)&lt;/span&gt; https://acefei.github.io
&amp;gt; Do you want to &lt;span class="nb"&gt;enable&lt;/span&gt; article pagination? &lt;span class="o"&gt;(&lt;/span&gt;Y/n&lt;span class="o"&gt;)&lt;/span&gt; n
&amp;gt; What is your &lt;span class="nb"&gt;time&lt;/span&gt; zone? &lt;span class="o"&gt;[&lt;/span&gt;Europe/Paris&lt;span class="o"&gt;]&lt;/span&gt; Asia/Shanghai
&amp;gt; Do you want to generate a Fabfile/Makefile to automate generation and publishing? &lt;span class="o"&gt;(&lt;/span&gt;Y/n&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; Do you want an auto-reload &lt;span class="p"&gt;&amp;amp;&lt;/span&gt; simpleHTTP script to assist with theme and site development? &lt;span class="o"&gt;(&lt;/span&gt;Y/n&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; Do you want to upload your website using FTP? &lt;span class="o"&gt;(&lt;/span&gt;y/N&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; Do you want to upload your website using SSH? &lt;span class="o"&gt;(&lt;/span&gt;y/N&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; Do you want to upload your website using Dropbox? &lt;span class="o"&gt;(&lt;/span&gt;y/N&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; Do you want to upload your website using S3? &lt;span class="o"&gt;(&lt;/span&gt;y/N&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; Do you want to upload your website using Rackspace Cloud Files? &lt;span class="o"&gt;(&lt;/span&gt;y/N&lt;span class="o"&gt;)&lt;/span&gt;
&amp;gt; Do you want to upload your website using GitHub Pages? &lt;span class="o"&gt;(&lt;/span&gt;y/N&lt;span class="o"&gt;)&lt;/span&gt; Y
&amp;gt; Is this your personal page &lt;span class="o"&gt;(&lt;/span&gt;username.github.io&lt;span class="o"&gt;)&lt;/span&gt;? &lt;span class="o"&gt;(&lt;/span&gt;y/N&lt;span class="o"&gt;)&lt;/span&gt; Y
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="write-first-post"&gt;&lt;a href="http://docs.getpelican.com/en/3.6.3/content.html"&gt;Write first post&lt;/a&gt;&lt;a class="headerlink" href="#write-first-post" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;To facilitate blog creation, I write &lt;a href="https://raw.githubusercontent.com/acefei/acefei.github.io/pelican/create_new_blog.sh"&gt;a script&lt;/a&gt; for creating the template with md format.&lt;/p&gt;
&lt;p&gt;Generate HTML pages and pre-view via http://localhost:8000/&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make html &amp;amp;&amp;amp; make serve&amp;amp;
firefox http://localhost:8000/
# After pre-view 
fg
# Then, Ctrl+C to terminate the process
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="publish"&gt;Publish&lt;a class="headerlink" href="#publish" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If everything is OK, generate the website.       &lt;br/&gt;
Currently, all pelican settings that are used to render HTML are on pelican branch.     &lt;br/&gt;
As previously mentioned, the static website content should be pulish from master branch.     &lt;br/&gt;
So, I need to publish respectively:    &lt;br/&gt;
For static website: (on &lt;a href="https://github.com/acefei/acefei.github.io/tree/master"&gt;master branch&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;make github
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For pelican settings: (on &lt;a href="https://github.com/acefei/acefei.github.io/tree/pelican"&gt;pelican branch&lt;/a&gt;)       &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;echo -e "*.pyc\noutput/" &amp;gt;&amp;gt; .gitignore
git add .
git commit -m "commit pelican setting"
git push -u origin pelican
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="extension"&gt;Extension&lt;a class="headerlink" href="#extension" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;h4 id="theme"&gt;Theme&lt;a class="headerlink" href="#theme" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Clone your fevorite &lt;a href="http://pelicanthemes.com/"&gt;theme&lt;/a&gt;, such as &lt;a href="http://oncrashreboot.com/elegant-best-pelican-theme-features"&gt;elegant&lt;/a&gt; &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;mkdir pelican-themes
cd pelican-themes
git clone git://github.com/talha131/pelican-elegant.git
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, add something like this to  pelicanconf.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;THEME = "pelican-themes/pelican-elegant"
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Note:          &lt;br/&gt;
1. Under GFW, we need to find an alternative CDN site to replace googleapis in theme folder.
2. If you want to add the theme into your pelican branch, remove the .git* path under the theme folder.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id="plugin"&gt;Plugin&lt;a class="headerlink" href="#plugin" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;Clone the plugin repo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone git://github.com/getpelican/pelican-plugins.git
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, add following contents into pelicanconf.py&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;###### plugin configuration #######
PLUGIN_PATHS = ['pelican-plugins']
PLUGINS = ['sitemap', 'extract_toc', 'tipue_search']
SITEMAP = {
'format': 'xml',
'priorities': {
    'articles': 0.5,
    'indexes': 0.5,
    'pages': 0.5
    },
'changefreqs': {
    'articles': 'weekly',
    'indexes': 'daily',
    'pages': 'monthly'
    }
}
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Ok, plugin install completely.&lt;/p&gt;
&lt;h4 id="pelican-settings"&gt;Pelican settings&lt;a class="headerlink" href="#pelican-settings" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;There are enhancements in &lt;a href="https://github.com/acefei/acefei.github.io/blob/pelican/pelicanconf.py"&gt;pelicanconfig.py&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="reference"&gt;Reference&lt;a class="headerlink" href="#reference" title="Permanent link"&gt;¶&lt;/a&gt;&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Blog &lt;a href="http://oncrashreboot.com"&gt;&lt;code&gt;onCrash=Reboot();&lt;/code&gt;&lt;/a&gt; uses Elegant theme. You
can see its configuration files at
&lt;a href="https://github.com/talha131/onCrashReboot"&gt;Github&lt;/a&gt; for inspiration. &lt;/p&gt;
&lt;/blockquote&gt;</content><category term="python"></category><category term="pelican"></category></entry></feed>