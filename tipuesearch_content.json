{"pages":[{"url":"how-to-stop-scrapy-redis-spider-when-its-idle.html","text":"scrapy-redis是以redis为基础的组件替换了原本scrapy的部分功能，让它可以分布式运作。 但是在使用的时候发现，它一旦待爬队列为空，spider不会自动结束，而是一直在等待redis push新的urls，在log末尾里可以看到如下内容： 2017-11-21 08:15:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 在scrapy-redis的 README 中得知: # Max idle time to prevent the spider from being closed when distributed crawling. # This only works if queue class is SpiderQueue or SpiderStack, # and may also block the same time when your spider start at the first time (because the queue is empty). SCHEDULER_IDLE_BEFORE_CLOSE = 10 添加到settings后，发现spider还是不会退出，只是不停的报exception。 重新想办法，既然log中能知道spider的状态，那我们就再加一个判断，连续出现X次scrapyed 0 itmes就退出不就好了么。 继续研读源码发现，这段log是scrapy extensions实现的，而且scrapy支持自定义extensions。 照着logsstats实现一个 close spider extension 并添加如下配置到settings中： EXTENSIONS = { 'prototypes.extensions.CloseSpiderRedis': 100, }, CLOSE_SPIDER_AFTER_IDLE_TIMES = 5 Done！ 2017-11-21 08:15:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-11-21 08:16:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-11-21 08:17:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-11-21 08:18:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-11-21 08:19:10 [scrapy.extensions.logstats] INFO: Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min) 2017-11-21 08:19:10 [scrapy.core.engine] INFO: Closing spider (close spider after 5 times of spider idle) 2017-11-21 08:19:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_bytes': 493, 'downloader/request_count': 2, 'downloader/request_method_count/GET': 2, 'downloader/response_bytes': 23397, 'downloader/response_count': 2, 'downloader/response_status_count/200': 1, 'downloader/response_status_count/302': 1, 'finish_reason': 'close spider after 5 times of spider idle', 'finish_time': datetime.datetime(2017, 11, 21, 8, 19, 10, 770725), 'log_count/DEBUG': 6, 'log_count/INFO': 14, 'memusage/max': 936964096, 'memusage/startup': 936964096, 'response_received_count': 1, 'scheduler/dequeued/redis': 2, 'scheduler/enqueued/redis': 2, 'start_time': datetime.datetime(2017, 11, 21, 8, 13, 10, 735722)} 2017-11-21 08:19:10 [scrapy.core.engine] INFO: Spider closed (close spider after 5 times of spider idle) 最后要注意，默认scrapy-redis退出后，会清掉requests/dupefilter的内容，如果你想保留配置，请务必在settings中加上： # Don't cleanup redis queues, allows to pause/resume crawls. SCHEDULER_PERSIST = True","tags":"Development","title":"How to stop scrapy-redis spider when it's idle"},{"url":"shi-yong-scrapydhe-scrapyd-clientbu-shu-pa-chong.html","text":"基本介绍 Scrapyd Scrapyd is a service for running Scrapy spiders. It allows you to deploy your Scrapy projects and control their spiders using a HTTP JSON API. Scrapyd can manage multiple projects and each project can have multiple versions uploaded, but only the latest one will be used for launching new spiders. Scrapyd-client Scrapyd-client is a client for Scrapyd. It provides the general scrapyd-client and the scrapyd-deploy utility which allows you to deploy your project to a Scrapyd server. 安装 pip install scrapyd scrapyd-client 配置服务器信息 当我们使用scrapy startnewproject来创建新工程时，会自动生成一个scrapy.cfg文件。 # Automatically created by: scrapy startproject # # For more information about the [deploy] section see: # https://scrapyd.readthedocs.org/en/latest/deploy.html [settings] default = prototypes.settings [deploy:proto_server] url = http://localhost:6800/ project = prototypes 其中有两个地方是需要我们更新的： proto_server是自定义的服务器名称 url是指scrapyd启动后，默认的服务器地址 启动scrapyd服务 执行scrapyd，然后用浏览器打开http://localhost:6800/查看界面是否启动成功 部署project 部署有两种方式： 1. 通过json API: addversion.json curl http://localhost:6800/addversion.json -F project=prototypes -F version=r23 -F egg=@prototypes.egg 其中egg需要用scrapyd-deploy来生成 # 进入scrapy工程目录 scrapyd-deploy --build-egg prototypes.egg 通过scrapyd-deploy一键部署 scrapyd-deploy proto_server -p prototypes 部署好后，我们可以用命令查看 scrapyd-deploy -L proto_server 然后在http://localhost:6800/查看界面发现Available projects： prototypes 运行spider curl http://localhost:6800/schedule.json -d project=prototypes -d spider=qichacha Note: 请务必确认在spider脚本中的初始化中对父类也进行初始化，否则会报错 exceptions.TypeError: __init__() got an unexpected keyword argument '_job' Further [ ] 集成 DormyMo/SpiderKeeper 或 Gerapy/Gerapy","tags":"Development","title":"使用Scrapyd和Scrapyd-client部署爬虫"},{"url":"zai-centos7shang-shi-yong-pythoncao-zuo-mysqlshi-jian.html","text":"Centos7上安装mysql sudo yum install mysql mysql-devel # centos7上已经使用mariadb替换了mysql，它们仅在安装配置上有些区别，使用上没有差别。 sudo yum install mariadb-server 配置mysql # 第一次启动会检查/var/lib/mysql目录是否为空，不为空将返回失败 sudo service mariadb start # 设置mysql密码 sudo mysql_secure_installation 如果配置时有任何错误，可在/var/log/mariadb/mariadb.log里查看具体原因 Python Mysql库安装 sudo yum install python-devel sudo pip install mysql-python # Toolkit for Python-based database access. sudo pip install dataset dataset常见操作 dataset 库是对SQLAlchemy二次封装，使其操作数据库就像操作json文件一样方便。 连接Mysql db = dataset.connect('mysql://username:password@port/dbname?charset=utf8') 添加记录 data = { 'key': 1, 'column2': 2, 'column3': 3, } # 插入记录，如果key，存在则报错 db[t_name].insert(data, ['key']) # 插入记录，如果key，存在则更新 db[t_name].upsert(data, ['key']) 查找记录 # 打印表的字段 print self.db['t_name'].columns # 打印表中所有数据 for row_data in self.db['t_name'].all(): print row_data # 根据多个条件找记录, 返回的是列表 self.db['t_name'].find(key1 = 1, key2 =2) # 返回一条记录 self.db['t_name'].find_one(key1 = 1, key2 =2) 考虑到find()查找性能，也可以使用原生的sql语句来进行query select * from t_name limit 99 更新记录 data = { 'key': 1, 'column2': 2, 'column3': 3, } self.db['t_name'].update(data, ['key']) 删除记录 self.db['t_name'].delete(key1=1, key2=2) # 删除所有记录 self.db['t_name'].delete() 关于dataset导出数据 现在dataset被分离成两个模块，如果想导出数据请看 datafreeze 详细用法请见 dataset readthedocs","tags":"Development","title":"在Centos7上使用Python操作Mysql实践"},{"url":"ru-he-zai-unittest-caseszhi-jian-chuan-di-bian-liang.html","text":"想必大家在写Python UT的时候，偶尔会遇到一些测试用例需要同时测试CRUD的接口。 这时需要在test cases之间传递变量该如何做呢？ 一开始，我们想既然是在一个class里，直接给instance增加attribute不就好了么? import unittest class TestA ( unittest . TestCase ): def __init__ ( self , testname ): super ( TestA , self ) . __init__ ( testname ) self . transmit_var = None def test_1 ( self ): self . transmit_var = 'hello world' def test_2 ( self ): print self . transmit_var if __name__ == \"__main__\" : unittest . main ( verbosity = 2 ) 结果...想当然了。 test_1 (__main__.TestA) ... ok test_2 (__main__.TestA) ... None ok ---------------------------------------------------------------------- Ran 2 tests in 0.000s OK 知识点一：在unittest中, 执行每个testcase的时候，会刷新instance attribute 既然self不行，那cls呢？ 那我们再想想unittest在执行class级别的setup，teardown的时候，其中的class attribute是不是对每个testcase都生效。 通过查看源码可以知道self. class is cls。 import unittest class TestA ( unittest . TestCase ): def test_1 ( self ): self . __class__ . transmit_var = 'hello world' def test_2 ( self ): print self . __class__ . transmit_var if __name__ == \"__main__\" : unittest . main ( verbosity = 2 ) Bingo！ test_1 (__main__.TestA) ... ok test_2 (__main__.TestA) ... hello world ok ---------------------------------------------------------------------- Ran 2 tests in 0.000s OK 知识点二：在unittest中，绑定在class的attribute可以在testcase之间传递 温故而知新： Class and Instance Attributes","tags":"Development","title":"如何在unittest cases之间传递变量"},{"url":"bloomfilter-for-scrapy-redis.html","text":"Summary: This article will illustrate how to renovate scrapy-redis to dupefilter. Why use bloomfilter Tips on optimizing scrapy for a high performance How to integrate bloomfilter into scrapy redis copy scrapy_redis into the path alongside settings in scrapy project implement bloom_filter.py in scrapy_redis path the code in https://github.com/acefei/scrapy-redis-docker/blob/master/scrapy_redis_demo/scrapy_redis_demo/bloom_filter.py modify scrapy_redis/dupefilter.py def __init__(self, server, key, debug=False): \"\"\"Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. \"\"\" self.server = server self.key = key self.debug = debug self.logdupes = True # 集成布隆过滤器 self.bf = BloomFilter(conn=server, key=key) # 利用连接池连接Redis def request_seen(self, request): \"\"\"Returns True if request was already seen. Parameters ---------- request : scrapy.http.Request Returns ------- bool \"\"\" # 集成布隆过滤器 if self.bf.is_exist(fp): # 判断如果域名在Redis里存在 return True else: self.bf.add(fp) # 如果不存在，将域名添加到Redis return False #fp = self.request_fingerprint(request) # This returns the number of values added, zero if already exists. #added = self.server.sadd(self.key, fp) #return added == 0 Add scrapy_redis configration into settings.py Note: use our scrapy_redis code like .scrapy_redis instead of scrapy_redis that created by pip install ################################## # Configuration for scrapy-redis { DUPEFILTER_CLASS = \"scrapy_redis_demo.scrapy_redis.dupefilter.RFPDupeFilter\" SCHEDULER = \"scrapy_redis_demo.scrapy_redis.scheduler.Scheduler\" SCHEDULER_PERSIST = True ITEM_PIPELINES = { 'scrapy_redis_demo.scrapy_redis.pipelines.RedisPipeline': 400, } # if u add 'network_mode: \"host\"' in scraper service in docker-compose.yaml # use host ip to access redis server REDIS_HOST = '172.16.100.62' # else use redis hostname to access redis server #REDIS_HOST = 'redis' REDIS_PORT = 6379 # Specify your redis uri # the uri scheme syntax: http://www.iana.org/assignments/uri-schemes/prov/redis #REDIS_URL = 'redis://172.16.100.62:6379' # } ################################## import .scrapy_redis in your spiders/xxxxx.py from scrapy_redis_demo.scrapy_redis.spiders import RedisCrawlSpider","tags":"Development","title":"BloomFilter For Scrapy Redis"},{"url":"web-scraping-practice.html","text":"Summary Recently, I found a TOEIC test website with enomous mp3 materials. Just in time, I decided to scrape and download MP3 in parallel by python Requirement pip install lxml # https://github.com/kennethreitz/grequests pip install grequests HTML Scraping lxml is a pretty extensive library written for parsing XML and HTML documents very quickly, even handling messed up tags in the process. We will also be using the Requests module instead of the already built-in urllib2 module due to improvements in speed and readability. About the details, please refer to the link Concurrency We will use GRequests which allows you to use Requests with Gevent to make asynchronous HTTP Requests easily. Or use the fork: FRequests The following need to be noted: 1. Using pool to limit concurrency. GRequests doesn't use pool by default, please see below code snippet: def map(requests, stream=False, size=None, exception_handler=None, gtimeout=None): \"\"\"Concurrently converts a list of Requests to Responses. :param requests: a collection of Request objects. :param stream: If True, the content will not be downloaded immediately. :param size: Specifies the number of requests to make at a time. If None, no throttling occurs. :param exception_handler: Callback function, called when exception occured. Params: Request, Exception :param gtimeout: Gevent joinall timeout in seconds. (Note: unrelated to requests timeout) \"\"\" requests = list(requests) pool = Pool(size) if size else None ... In my requirement, there are enomous links need to settle. If don't constrain concurrency, it would lead to the exception \"gevent.hub.LoopExit: This operation would block forever\". 2. Using session to avoid the fd consuming. There is a same problem with me. Finally, the source code is available in the gist SEE ALSO: http://xlambda.com/gevent-tutorial/","tags":"Development","title":"Web Scraping Practice"},{"url":"make-a-github-pages-blog-with-pelican.html","text":"Make a Github Pages blog with Pelican: Install dependence sudo yum install - y git sudo pip install pelican markdown ghp - import BeautifulSoup4 Create user pages There are two basic types of GitHub Pages: User/Organization Pages and Project Pages . Generally, most people will select User Pages, and there are two caveat as below: You must use the username.github.io naming scheme. Content from the master branch will be used to build and publish your GitHub Pages site. When User Pages are built, they are available at http(s)://username.github.io. Set up the blog with Pelican Create a new branch (pelican) for hosting pelican settings on github, please refer to Publish $ git clone https://github.com/acefei/acefei.github.io $ cd acefei.github.io/ $ git checkout -b pelican $ pelican-quickstart Welcome to pelican-quickstart v3.7.1. This script will help you create a new Pelican-based website. Please answer the following questions so this script can generate the files needed by Pelican. > Where do you want to create your new web site? [ . ] > What will be the title of this web site? Acefei ' s Blog > Who will be the author of this web site? acefei > What will be the default language of this web site? [ en ] > Do you want to specify a URL prefix? e.g., http://example.com ( Y/n ) > What is your URL prefix? ( see above example ; no trailing slash ) https://acefei.github.io > Do you want to enable article pagination? ( Y/n ) n > What is your time zone? [ Europe/Paris ] Asia/Shanghai > Do you want to generate a Fabfile/Makefile to automate generation and publishing? ( Y/n ) > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? ( Y/n ) > Do you want to upload your website using FTP? ( y/N ) > Do you want to upload your website using SSH? ( y/N ) > Do you want to upload your website using Dropbox? ( y/N ) > Do you want to upload your website using S3? ( y/N ) > Do you want to upload your website using Rackspace Cloud Files? ( y/N ) > Do you want to upload your website using GitHub Pages? ( y/N ) Y > Is this your personal page ( username.github.io ) ? ( y/N ) Y Write first post To facilitate blog creation, I write a script for creating the template with md format. Generate HTML pages and pre-view via http://localhost:8000/ make html && make serve& firefox http://localhost:8000/ # After pre-view fg # Then, Ctrl+C to terminate the process Publish If everything is OK, generate the website. Currently, all pelican settings that are used to render HTML are on pelican branch. As previously mentioned, the static website content should be pulish from master branch. So, I need to publish respectively: For static website: (on master branch ) make github For pelican settings: (on pelican branch ) echo -e \"*.pyc\\noutput/\" >> .gitignore git add . git commit -m \"commit pelican setting\" git push -u origin pelican Extension Theme Clone your fevorite theme , such as elegant mkdir pelican-themes cd pelican-themes git clone git://github.com/talha131/pelican-elegant.git Then, add something like this to pelicanconf.py THEME = \"pelican-themes/pelican-elegant\" Note: 1. Under GFW, we need to find an alternative CDN site to replace googleapis in theme folder. 2. If you want to add the theme into your pelican branch, remove the .git* path under the theme folder. Plugin Clone the plugin repo. git clone git://github.com/getpelican/pelican-plugins.git Then, add following contents into pelicanconf.py ###### plugin configuration ####### PLUGIN_PATHS = ['pelican-plugins'] PLUGINS = ['sitemap', 'extract_toc', 'tipue_search'] SITEMAP = { 'format': 'xml', 'priorities': { 'articles': 0.5, 'indexes': 0.5, 'pages': 0.5 }, 'changefreqs': { 'articles': 'weekly', 'indexes': 'daily', 'pages': 'monthly' } } Ok, plugin install completely. Pelican settings There are enhancements in pelicanconfig.py Reference Blog onCrash=Reboot(); uses Elegant theme. You can see its configuration files at Github for inspiration.","tags":"Development","title":"Make a Github Pages blog with Pelican"}]}