{"pages":[{"title":"\\K magic in regex","text":"First of all, Let us see a question, how to get 123 789 from the string abc: 123 def 456 ghi: 789 by shell one liner. The elegant way I can figure out is to use lookbehind assertion (?<=...) . $ echo \"abc: 123 def 456 ghi: 789\" | grep - Po '(?<=:\\s)\\d+' 123 789 However, the string is changed to abc: 123 def 456 ghi: 789 with multiple spaces after : , lookbehind assertion won't work in this case, as it only supports fixed length pattern. $ echo \"abc: 123 def 456 ghi: 789\" | grep - Po '(?<=:\\s+)\\d+' grep : lookbehind assertion is not fixed length Now we just need to a small change, to use \\K instead of (?<=...) $ echo \"abc: 123 def 456 ghi: 789\" | grep - Po ':\\s*\\K\\d+' 123 789 Further reading Understanding \\G and \\K in regex","tags":"Development","url":"k-magic-in-regex.html"},{"title":"Create user with root privilege for storage server","text":"Qnap Ensure SSH is enable on Qnap website . ContrlPanel -> Network & File Services -> Telnet/SSH -> Allow SSH connection ssh to Qnap server and run below cmd. adduser --gid 0 admin-last sed - i '/&#94;admin/aadmin-last ALL=(ALL) ALL' / usr / etc / sudoers passwd admin - last NetApp Enable ssh on netapp website Secure Admin -> SSH -> Enable/Disable Add below into ~/.ssh/config Host * KexAlgorithms + diffie - hellman - group1 - sha1 Ciphers + 3 des - cbc ssh to Netapp and run below cmd. useradmin user add root - last - c \" adminstrator for netapp last resort \" - g Administrators In NetApp ONTAP, it might be failed as \"useradmin user add\" is not supported: use the \"security login create\" command. As the NetApp ONTAP Documentation , We can use below cmd. # find current vserver name cluster1 :: > security login show cluster1 :: > security login create - vserver vserver - name - user - or - group - name admin - last - application ssh - authentication - method password - role admin FreeNAS ssh to FreeNAS. add user with root privilege (set the user group is wheel) # adduser Username : root - last Full name : Uid ( Leave empty for default ) : Login group [ root - last ]: wheel Login group is wheel . Invite root - last into other groups ? []: Login class [ default ]: Shell ( sh csh tcsh bash rbash netcli . sh ksh93 mksh zsh rzsh scponly git - shell nologin ) [ sh ]: Home directory [ / home / root - last ]: Home directory permissions ( Leave empty for default ) : Use password - based authentication ? [ yes ]: Use an empty password ? ( yes / no ) [ no ]: Use a random password ? ( yes / no ) [ no ]: Enter password : Enter password again : Lock out the account after creation ? [ no ]: Username : root - last Password : ***** Full Name : Uid : 1001 Class : Groups : wheel Home : / home / root - last Home Mode : Shell : / bin / sh Locked : no OK ? ( yes / no ) : yes # echo \" %wheel ALL=(ALL) ALL \" >> / etc / local / sudoers","tags":"DevOps","url":"create-user-with-root-privilege-for-storage-server.html"},{"title":"Data processing with pipe","text":"现如今云计算，大数据流式处理都会涉及到MapReduce，pipeline等概念，而 《左耳朵耗子：什么是函数式编程？》 对其深入浅出，尤其是最后一段Pipe相关的代码，very graceful and elegent！ 那么这篇文章也将练习一下Pipe的用法。 首先，照着耗子哥文章，先来实现一个Pipe装饰器类 import functools class Pipe : def __init__ ( self , func ): self . func = func functools . update_wrapper ( self , func ) def __ror__ ( self , pipe_left_obj ): return self . func ( pipe_left_obj ) def __call__ ( self , * args , ** kwargs ): def wrapped ( pipe_left_obj ): return self . func ( pipe_left_obj , * args , ** kwargs ) return Pipe ( wrapped ) 这里用到的 spacial method __ror__ 是重载了 | 运算符. 注意 __ror__ 和 __or__ 的区别，重载 __ror__ 是因为我们需要数据是从 | 的左边对象传给右边对象，比如 x | y 等于 y.__ror__(x) , 而 __or__ 则相反, 它等于 x.__or__(y) Pipe的用法示例： @ Pipe def to_str ( data , sep = ' , ' ) : return sep . join ( map ( str , data )) print [ 1 , 2 , 3 ] | to_str # output is ' 1,2,3 ' print [ 4 , 5 , 6 ] | to_str ( ' # ' ) # output is ' 1#2#3 ' 这里的 to_str('#') 会调用 Pipe.__call__() , 实现 __call__ 需要注意几点： 1. 定义的时候带上 (*args, **kwargs) 来接受 to_str 的参数。 2. 返回值应该是Pipe对象，用于 | 运算。 3. Pipe初始化的时候需要传入函数对象（wrapped）做参数，且此函数的第一个参数是用于接受 | 左边对象。 4. 在 __call__ 中的 self.func 是指的 function to_str , 而在 __ror__ 里的 self.func 则是指的 function wrapped 。 教的曲唱不得，为了深刻理解，最好还是自己在pycharm里用debug单步调试一下看看。 接下来我们尝试一下大数据里常遇到场景，假设有一段英文文章，我们对它统计词频并排序后打印分哪几步？ - 先将整段文章分割成单词 - 然后聚合 - 对聚合后的数据进行计数统计 - 根据规则进行排序 - 打印 import itertools @Pipe def split_to_words ( content ): return content . split () @Pipe def groupby ( iterable , keyfunc ): return itertools . groupby ( sorted ( iterable , key = keyfunc ), keyfunc ) @Pipe def mapping ( iterable , func ): returm ( func ( x ) for x in iterable ) @Pipe def count ( iterable ): return sum ( map ( lambda x : 1 , iterable )) @Pipe def sort ( iterable , ** kwargs ): return sorted ( iterable , ** kwargs ) @Pipe def echo ( iterable ): print iterable 我们拿《The Zen of Python》来试试效果： text = \"\"\" The Zen of Python , by Tim Peters Beautiful is better than ugly . Explicit is better than implicit . Simple is better than complex . Complex is better than complicated . Flat is better than nested . Sparse is better than dense . Readability counts . Special cases aren ' t special enough to break the rules. Although practicality beats purity . Errors should never pass silently . Unless explicitly silenced . In the face of ambiguity , refuse the temptation to guess . There should be one -- and preferably only one -- obvious way to do it . Although that way may not be obvious at first unless you ' re Dutch. Now is better than never . Although never is often better than * right * now . If the implementation is hard to explain , it ' s a bad idea. If the implementation is easy to explain , it may be a good idea . Namespaces are one honking great idea -- let ' s do more of those! \"\"\" text | split_to_words | groupby ( lambda x : x ) | mapping ( lambda x : ( x [ 0 ], x [ 1 ] | count )) | sort ( key = lambda x : x [ 1 ], reverse = True ) | echo 输出如下: [ ( ' is ' , 10 ) , ( ' better ' , 8 ) , ( ' than ' , 8 ) , ( ' the ' , 5 ) , ( ' to ' , 5 ) , ( ' Although ' , 3 ) , ( ' be ' , 3 ) , ( ' of ' , 3 ) , ( ' If ' , 2 ) , ( ' a ' , 2 ) , ( ' do ' , 2 ) , ( ' explain, ' , 2 ) , ( ' idea. ' , 2 ) , ( ' implementation ' , 2 ) , ( ' may ' , 2 ) , ( ' never ' , 2 ) , ( ' one ' , 2 ) , ( ' should ' , 2 ) , ( ' way ' , 2 ) , ( ' *right* ' , 1 ) , ( ' -- ' , 1 ) , ( ' --obvious ' , 1 ) , ( ' Beautiful ' , 1 ) , ( ' Complex ' , 1 ) , ( ' Dutch. ' , 1 ) , ( ' Errors ' , 1 ) , ( ' Explicit ' , 1 ) , ( ' Flat ' , 1 ) , ( ' In ' , 1 ) , ( ' Namespaces ' , 1 ) , ( ' Now ' , 1 ) , ( ' Peters ' , 1 ) , ( ' Python, ' , 1 ) , ( ' Readability ' , 1 ) , ( ' Simple ' , 1 ) , ( ' Sparse ' , 1 ) , ( ' Special ' , 1 ) , ( ' The ' , 1 ) , ( ' There ' , 1 ) , ( ' Tim ' , 1 ) , ( ' Unless ' , 1 ) , ( ' Zen ' , 1 ) , ( ' ambiguity, ' , 1 ) , ( ' and ' , 1 ) , ( ' are ' , 1 ) , ( \" aren't \" , 1 ) , ( ' at ' , 1 ) , ( ' bad ' , 1 ) , ( ' beats ' , 1 ) , ( ' break ' , 1 ) , ( ' by ' , 1 ) , ( ' cases ' , 1 ) , ( ' complex. ' , 1 ) , ( ' complicated. ' , 1 ) , ( ' counts. ' , 1 ) , ( ' dense. ' , 1 ) , ( ' easy ' , 1 ) , ( ' enough ' , 1 ) , ( ' explicitly ' , 1 ) , ( ' face ' , 1 ) , ( ' first ' , 1 ) , ( ' good ' , 1 ) , ( ' great ' , 1 ) , ( ' guess. ' , 1 ) , ( ' hard ' , 1 ) , ( ' honking ' , 1 ) , ( ' idea ' , 1 ) , ( ' implicit. ' , 1 ) , ( ' it ' , 1 ) , ( \" it's \" , 1 ) , ( ' it. ' , 1 ) , ( \" let's \" , 1 ) , ( ' more ' , 1 ) , ( ' nested. ' , 1 ) , ( ' never. ' , 1 ) , ( ' not ' , 1 ) , ( ' now. ' , 1 ) , ( ' obvious ' , 1 ) , ( ' often ' , 1 ) , ( ' one-- ' , 1 ) , ( ' only ' , 1 ) , ( ' pass ' , 1 ) , ( ' practicality ' , 1 ) , ( ' preferably ' , 1 ) , ( ' purity. ' , 1 ) , ( ' refuse ' , 1 ) , ( ' rules. ' , 1 ) , ( ' silenced. ' , 1 ) , ( ' silently. ' , 1 ) , ( ' special ' , 1 ) , ( ' temptation ' , 1 ) , ( ' that ' , 1 ) , ( ' those! ' , 1 ) , ( ' ugly. ' , 1 ) , ( ' unless ' , 1 ) , ( \" you're \" , 1 ) ] Works like a charm! 参考 简单地理解 Python 的装饰器 Python修饰器的函数式编程 函数式编程","tags":"Development","url":"data-processing-with-pipe.html"},{"title":"perl & cgi practice","text":"This article is inspired from an practical assignment , as well as I have prior knowledge of perl script, so just for fun to write this. Install and configure software sudo yum install - y httpd perl perl - CGI Modify httpd.conf Load cgi module find /etc/httpd/modules/ -iname \"*cgi*\" Then we can figure out below: /etc/httpd/modules/mod_cgi.so Let's add that to the /etc/httpd/conf/httpd.conf file: LoadModule cgi_module modules/mod_cgi.so Change the directory settings in httpd.conf Modify as below in /etc/httpd/conf/httpd.conf: <Directory \"/var/www/cgi-bin\" > Options +ExecCGI AddHandler cgi-script .cgi .pl </Directory> Create cgi script and Test it Create cgi script /var/www/cgi-bin/simple.pl: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #!/usr/bin/perl use CGI qw/:standard/ ; print header , start_html ( 'Simple Script' ), h1 ( 'Simple Script' ), start_form , \"What's your name? \" , textfield ( 'name' ), p , \"What's the combination?\" , checkbox_group ( - name => 'words' , - values => [ 'eenie' , 'meenie' , 'minie' , 'moe' ], - defaults => [ 'eenie' , 'moe' ]), p , \"What's your favorite color?\" , popup_menu ( - name => 'color' , - values => [ 'red' , 'green' , 'blue' , 'chartreuse' ]), p , submit , end_form , hr , \"\\n\" ; if ( param ) { print \"Your name is \" , em ( param ( 'name' )), p , \"The keywords are: \" , em ( join ( \", \" , param ( 'words' ))), p , \"Your favorite color is \" , em ( param ( 'color' )), \".\\n\" ; } print end_html ; And then you need to tell http server that these CGI scripts are allowed to be executed as programs. chmod 755 .cgi .pl Or you can target individual CGI scripts. chmod 755 simple.pl Invoking script in /var/www/cgi-bin to guarantee the output is fine. $ ./simple.pl Content-Type: text/html; charset=ISO-8859-1 <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"> <html xmlns= \"http://www.w3.org/1999/xhtml\" lang= \"en-US\" xml:lang= \"en-US\" > <head> <title> Simple Script </title> <meta http-equiv= \"Content-Type\" content= \"text/html; charset=iso-8859-1\" /> </head> <body> <h1> Simple Script </h1><form method= \"post\" action= \"http://localhost\" enctype= \"multipart/form-data\" > What's your name? <input type= \"text\" name= \"name\" /><p /> What's the combination? <label><input type= \"checkbox\" name= \"words\" value= \"eenie\" checked= \"checked\" /> eenie </label><label><input type= \"checkbox\" name= \"words\" value= \"meenie\" /> meenie </label><label><input type= \"checkbox\" name= \"words\" value= \"minie\" /> minie </label><label><input type= \"checkbox\" name= \"words\" value= \"moe\" checked= \"checked\" /> moe </label><p /> What's your favorite color? <select name= \"color\" > <option value= \"red\" > red </option> <option value= \"green\" > green </option> <option value= \"blue\" > blue </option> <option value= \"chartreuse\" > chartreuse </option> </select><p /><input type= \"submit\" name= \".submit\" /><div><input type= \"hidden\" name= \".cgifields\" value= \"words\" /></div></form><hr /> </body> </html> Now restart httpd like so: sudo systemctl restart httpd . service Finally, open the broswer with the url: http://localhost/cgi-bin/simple.pl Further Practice: Use javascript in Perl CGI 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #!/usr/bin/perl use CGI qw/:standard/ ; sub login_page { header , start_html ( 'Perl & CGI & JS' ), h3 ( 'Login' ), start_form , \"Username: \" , textfield ( 'usr' ), p , \"Password:\" , textfield ( 'pwd' ), p , submit , end_form , hr , \"\\n\" ; } sub is_textfield_empty { return ( param ( 'usr' ) eq '' or param ( 'pwd' ) eq '' ); } sub alert_and_redirect { my $err_msg = shift ; my $rdurl = shift ; my $js = << JS ; alert(\"$err_msg\"); window.location=\"$rdurl\"; JS print qq(<script type=\"text/javascript\">$js</script>) ; } sub printenv { foreach $var ( sort ( keys ( %ENV ))) { $val = $ENV { $var }; $val =~ s |\\ n |\\\\ n | g ; $val =~ s | \"|\\\\\" | g ; print p , \"${var}=\\\"${val}\\\"\" ; } } print login_page ; # When click submit button, it will send http POST method, otherwire send GET if ( $ENV { 'REQUEST_METHOD' } eq 'POST' ) { if ( is_textfield_empty ) { alert_and_redirect \"Please enter username and password!\" , url ; } else { print printenv ; } } print end_html ;","tags":"Development","url":"perl-cgi-practice.html"},{"title":"使用cookiecutter+pypackage-minimal快速制作一个python package","text":"Prerequisites pip install scrapy pip install cookiecutter https : // github . com / kragniz / cookiecutter - pypackage - minimal . git Start cookiecutter $ cookiecutter cookiecutter - pypackage - minimal / / usr / lib / python2 .7 / site - packages / requests / __init__ . py : 80 : RequestsDependencyWarning : urllib3 ( 1.22 ) or chardet ( 2.2.1 ) doesn ' t match a supported version ! RequestsDependencyWarning ) author_name [ Louis Taylor ] : acefei author_email [ louis @ kragniz . eu ] : acefei @163. com package_name [ cookiecutter_pypackage_minimal ] : scrapy_templates package_version [ 0.1.0 ] : package_description [ An opinionated , minimal cookiecutter template for Python packages ] : Templates for creating new projects with startproject command and new spiders with genspider command . package_url [ https : //github.com/borntyping/cookiecutter-pypackage-minimal]: https://github.com/acefei/scrapy_templates readme_pypi_badge [ True ] : readme_travis_badge [ True ] : False readme_travis_url [ https : //travis-ci.org/borntyping/cookiecutter-pypackage-minimal]: False $ ls scrapy_templates /* scrapy_templates/README.rst scrapy_templates/setup.py scrapy_templates/tox.ini scrapy_templates/scrapy_templates: __init__.py scrapy_templates/tests: __init__.py test_sample.py Create scrapy template Copy the original template from scrapy project. cd scrapy_templates / scrapy_templates / mkdir scrapy && cd scrapy cp - r / usr / lib64 / python2 . 7 / site - packages / scrapy / templates / project . Now, you can customize the template in scrapy_templates/scrapy_templates/scrapy Add non-package files Since the templates path is not a python package, to specify those files to distribute, we should add MANIFEST file. # in MANIFEST recursive - include scrapy_templates / * In order for these files to be copied at install time to the package's folder inside site-packages, you'll need to supply include_package_data=True to the setup() function. Command Line Script write a python script command_line.py with some functions in the package. register those functihons to setup() function. import setuptools setuptools . setup ( ... entry_points = { 'console_scripts' : [ 'scrapy-startproject=scrapy_templates.command_line:startproj' , 'scrapy-genspider=scrapy_templates.command_line:genspider' , ], } ... ) Specifying Dependencies Edit setup.py , we just add an install_requires keyword argument: import setuptools setuptools . setup ( ... install_requires = [ 'scrapy-redis' , 'scrapy-splash' , 'scrapy-random-useragent' , 'scrapy-redis-bloomfilter' , ], ... ) Summary Now, we have finished an python package. It's easy to install this package by pip. pip install git + https : // github . com / acefei / scrapy_templates . git About this package details, please find acefei/scrapy_templates Inspiration python-packaging","tags":"DevOps","url":"shi-yong-cookiecutterpypackage-minimalkuai-su-zhi-zuo-yi-ge-python-package.html"},{"title":"关于Selenium+PhantomJS设置HTTP PROXY的问题","text":"在写爬虫的时候，经常会遇到anti-spider，这时候我们可以采取切换代理ip来绕过限制。但是最近在Selenium+PhantomJS实践过程中遇到一个很trick的问题，在此做一下记录。 python requests with proxy 一开始我们还是用pythonic的方式来看看，http proxy是如何隐藏真实ip的。 import requests print \"--No Proxy--\" print requests . get ( \"http://httpbin.org/ip\" ) . content # 在网上随便找个免费http代理 proxy = '182.121.201.9:9999' print \"--Proxy {0}--\" . format ( proxy ) proxies = { \"http\" : \"http://{0}\" . format ( proxy ) } print requests . get ( \"http://httpbin.org/ip\" , proxies = proxies ) . content output: --No Proxy-- { \"origin\" : \"36.152.29.163\" } --Proxy 182.121.201.9:9999-- { \"origin\" : \"36.152.29.163, 182.121.204.0\" } 可以看出来origin的值插入了一条新的ip，证明设置proxy生效了。 但是我们要注意，上面使用的代理并不是高匿的，所以从origin里面还是能找到机器的真实ip(仍然会被反爬)。 高匿ip的输出应该是以下结果： --No Proxy-- { \"origin\" : \"36.152.29.163\" } --Proxy 113.218.218.86:808-- { \"origin\" : \"113.218.218.86\" } phantomjs with proxy 编辑httpbin_test.js， \" use strict \" ; var page = require ( ' webpage ' ) . create () ; page . open ( ' http://httpbin.org/ip ' , function ( status ) { if ( status !== ' success ' ) { console . log ( ' Unable to access network ' ) ; } else { console . log ( page . content ) ; } phantom . exit () ; } ) ; 执行phantomjs命令， $ phantomjs -- proxy = 182 . 121 . 201 . 9 : 9999 httpbin_test . js < html >< head ></ head >< body >< pre style = \" word-wrap: break-word; white-space: pre-wrap; \" > { \" origin \" : \" 36.152.29.163, 182.121.204.0 \" } </ pre ></ body ></ html > Ok，proxy设置成功。 selenium + phantomjs with proxy Stackoverflow上有人推荐方案， from selenium import webdriver from selenium.webdriver import DesiredCapabilities from selenium.webdriver.common.proxy import Proxy from selenium.webdriver.common.proxy import ProxyType proxy = Proxy ({ 'proxyType' : ProxyType . MANUAL , 'httpProxy' : '182.121.201.9:9999' # 代理ip和端口 }) desired_capabilities = DesiredCapabilities . PHANTOMJS . copy () proxy . add_to_capabilities ( desired_capabilities ) print \"add proxy\" + proxy . http_proxy driver = webdriver . PhantomJS ( desired_capabilities = desired_capabilities ) driver . get ( 'http://httpbin.org/ip' ) print driver . page_source driver . close () output： add proxy182 . 121 . 201 . 9 : 9999 < html >< head ></ head >< body >< pre style = \" word-wrap: break-word; white-space: pre-wrap; \" > { \" origin \" : \" 36.152.29.163 \" } </ pre ></ body ></ html > 奇怪，这里发现proxy并没有生效，不知道是不是我的phantomjs 1.9.7版本的bug。 那换个方案试试， from selenium import webdriver service_args = [ '--proxy=182.121.201.9:9999' , ] driver = webdriver . PhantomJS ( service_args = service_args ) driver . get ( 'http://httpbin.org/ip' ) print driver . page_source driver . close () output： < html >< head ></ head >< body >< pre style = \" word-wrap: break-word; white-space: pre-wrap; \" > { \" origin \" : \" 36.152.29.163, 182.121.204.0 \" } </ pre ></ body ></ html > Ok, proxy设置成功了。 可以看出来，通过service_args传递参数其实跟phantomjs命令行传参的是一样效果。 那么，如果想使用带auth的代理，只要加上对应的option就好了，很方便。 $ phantomjs - h Usage : phantomjs [ switchs ] [ options ] [ script ] [ argument [ argument [...]]] Options : -- cookies - file =< val > Sets the file name to store the persistent cookies -- config =< val > Specifies JSON - formatted configuration file -- debug =< val > Prints additional warning and debug message : ' true ' or ' false ' ( default ) -- disk - cache =< val > Enables disk cache : ' true ' or ' false ' ( default ) -- ignore - ssl - errors =< val > Ignores SSL errors ( expired / self - signed certificate errors ) : ' true ' or ' false ' ( default ) -- load - images =< val > Loads all inlined images : ' true ' ( default ) or ' false ' -- local - storage - path =< val > Specifies the location for offline local storage -- local - storage - quota =< val > Sets the maximum size of the offline local storage ( in KB ) -- local - to - remote - url - access =< val > Allows local content to access remote URL : ' true ' or ' false ' ( default ) -- max - disk - cache - size =< val > Limits the size of the disk cache ( in KB ) -- output - encoding =< val > Sets the encoding for the terminal output , default is ' utf8 ' -- remote - debugger - port =< val > Starts the script in a debug harness and listens on the specified port -- remote - debugger - autorun =< val > Runs the script in the debugger immediately : ' true ' or ' false ' ( default ) -- proxy =< val > Sets the proxy server , e . g . ' --proxy=http://proxy.company.com:8080 ' -- proxy - auth =< val > Provides authentication information for the proxy , e . g . '' - proxy - auth = username : password ' -- proxy - type =< val > Specifies the proxy type , ' http ' ( default ) , ' none ' ( disable completely ) , or ' socks5 ' -- script - encoding =< val > Sets the encoding used for the starting script , default is ' utf8 ' -- web - security =< val > Enables web security , ' true ' ( default ) or ' false ' -- ssl - protocol =< val > Sets the SSL protocol ( supported protocols : ' SSLv3 ' ( default ) , ' SSLv2 ' , ' TLSv1 ' , ' any ' ) -- ssl - certificates - path =< val > Sets the location for custom CA certificates ( if none set , uses system default ) -- webdriver =< val > Starts in ' Remote WebDriver mode ' ( embedded GhostDriver ) : ' [[<IP>:]<PORT>] ' ( default ' 127.0.0.1:8910 ' ) -- webdriver - logfile =< val > File where to write the WebDriver ' s Log (default ' none ' ) (NOTE: needs ' -- webdriver ' ) -- webdriver - loglevel =< val > WebDriver Logging Level : ( supported : ' ERROR ' , ' WARN ' , ' INFO ' , ' DEBUG ' ) ( default ' INFO ' ) ( NOTE : needs ' --webdriver ' ) -- webdriver - selenium - grid - hub =< val > URL to the Selenium Grid HUB : ' URL_TO_HUB ' ( default ' none ' ) ( NOTE : needs ' --webdriver ' ) - w , -- wd Equivalent to ' --webdriver ' option above - h , -- help Shows this message and quits - v , -- version Prints out PhantomJS version Any of the options that accept boolean values ( ' true ' / ' false ' ) can also accept ' yes ' / ' no ' . Without any argument , PhantomJS will launch in interactive mode ( REPL ) . Documentation can be found at the web site , http : // phantomjs . org .","tags":"Development","url":"guan-yu-seleniumphantomjsshe-zhi-http-proxyde-wen-ti.html"},{"title":"机器学习资料整理(Python方向，建议通读)","text":"一些好东西，入门前未必看得懂，要等学有小成时再看才能体会。 Python 致Python初学者：Anaconda入门使用指南 ——选择anaconda，体验拎包入住的感觉 数据处理 numpy Numpy--Numerical Python，一个基于Python的可以存储和处理大型矩阵的库。几乎是Python 生态系统的数值计算的基石，例如Scipy，Pandas，Scikit-learn，Keras等都基于Numpy。 Quickstart Index 查询api，推荐！ Scipy SciPy函数库在NumPy库的基础上增加了众多的数学、科学以及工程计算中常用的库函数。例如线性代数、常微分方程数值求解、信号处理、图像处理、稀疏矩阵等等。 Scipy Lecture Notes 极力推荐的一个学习笔记！ pandas pandas是基于Numpy构建的含有更高级数据结构和工具的数据分析包。 类似于Numpy的核心是ndarray，pandas也是围绕着Series和DataFrame两个核心数据结构展开的。 Series和DataFrame分别对应于一维的序列和二维的表结构。 10 Minutes to pandas ( 翻译 ) Index 查询api，推荐！ 机器算法 scikit-learn 数据可视化 Seaborn Seaborn tutorial Matplotlib Matplotlib tutorial ( 翻译 ) Pyecharts 如果想使用pandas和numpy数据，需要转换一下。参考： 增加了对 Pandas 和 Numpy 数据的简单处理 扩展阅读 python机器学习入门资料梳理 python 线性代数 用numpy来讲解 Python 统计分析 pandas教程 numpy函数 awesome-machine-learning","tags":"Development","url":"ji-qi-xue-xi-zi-liao-zheng-li-pythonfang-xiang-jian-yi-tong-du.html"},{"title":"How to stop scrapy-redis spider when it's idle","text":"scrapy-redis是以redis为基础的组件替换了原本scrapy的部分功能，让它可以分布式运作。 但是在使用的时候发现，它一旦待爬队列为空，spider不会自动结束，而是一直在等待redis push新的urls，在log末尾里可以看到如下内容： 2017 - 11 - 21 08 : 15 : 10 [ scrapy . extensions . logstats ] INFO : Crawled 1 pages ( at 0 pages / min ), scraped 0 items ( at 0 items / min ) 在scrapy-redis的 README 中得知: # Max idle time to prevent the spider from being closed when distributed crawling . # This only works if queue class is SpiderQueue or SpiderStack , # and may also block the same time when your spider start at the first time ( because the queue is empty ) . SCHEDULER_IDLE_BEFORE_CLOSE = 10 添加到settings后，发现spider还是不会退出，只是不停的报exception。 重新想办法，既然log中能知道spider的状态，那我们就再加一个判断，连续出现X次scrapyed 0 itmes就退出不就好了么。 继续研读源码发现，这段log是scrapy extensions实现的，而且scrapy支持自定义extensions。 照着logsstats实现一个 close spider extension 并添加如下配置到settings中： EXTENSIONS = { 'prototypes.extensions.CloseSpiderRedis' : 100 , } , CLOSE_SPIDER_AFTER_IDLE_TIMES = 5 Done！ 2017 - 11 - 21 08 : 15 : 10 [ scrapy . extensions . logstats ] INFO : Crawled 1 pages ( at 0 pages / min ), scraped 0 items ( at 0 items / min ) 2017 - 11 - 21 08 : 16 : 10 [ scrapy . extensions . logstats ] INFO : Crawled 1 pages ( at 0 pages / min ), scraped 0 items ( at 0 items / min ) 2017 - 11 - 21 08 : 17 : 10 [ scrapy . extensions . logstats ] INFO : Crawled 1 pages ( at 0 pages / min ), scraped 0 items ( at 0 items / min ) 2017 - 11 - 21 08 : 18 : 10 [ scrapy . extensions . logstats ] INFO : Crawled 1 pages ( at 0 pages / min ), scraped 0 items ( at 0 items / min ) 2017 - 11 - 21 08 : 19 : 10 [ scrapy . extensions . logstats ] INFO : Crawled 1 pages ( at 0 pages / min ), scraped 0 items ( at 0 items / min ) 2017 - 11 - 21 08 : 19 : 10 [ scrapy . core . engine ] INFO : Closing spider ( close spider after 5 times of spider idle ) 2017 - 11 - 21 08 : 19 : 10 [ scrapy . statscollectors ] INFO : Dumping Scrapy stats : { 'downloader/request_bytes' : 493 , 'downloader/request_count' : 2 , 'downloader/request_method_count/GET' : 2 , 'downloader/response_bytes' : 23397 , 'downloader/response_count' : 2 , 'downloader/response_status_count/200' : 1 , 'downloader/response_status_count/302' : 1 , 'finish_reason' : 'close spider after 5 times of spider idle' , 'finish_time' : datetime . datetime ( 2017 , 11 , 21 , 8 , 19 , 10 , 770725 ), 'log_count/DEBUG' : 6 , 'log_count/INFO' : 14 , 'memusage/max' : 936964096 , 'memusage/startup' : 936964096 , 'response_received_count' : 1 , 'scheduler/dequeued/redis' : 2 , 'scheduler/enqueued/redis' : 2 , 'start_time' : datetime . datetime ( 2017 , 11 , 21 , 8 , 13 , 10 , 735722 ) } 2017 - 11 - 21 08 : 19 : 10 [ scrapy . core . engine ] INFO : Spider closed ( close spider after 5 times of spider idle ) 最后要注意，默认scrapy-redis退出后，会清掉requests/dupefilter的内容，如果你想保留配置，请务必在settings中加上： # Don ' t cleanup redis queues, allows to pause/resume crawls. SCHEDULER_PERSIST = True","tags":"Development","url":"how-to-stop-scrapy-redis-spider-when-its-idle.html"},{"title":"使用Scrapyd和Scrapyd-client部署爬虫","text":"基本介绍 Scrapyd Scrapyd is a service for running Scrapy spiders. It allows you to deploy your Scrapy projects and control their spiders using a HTTP JSON API. Scrapyd can manage multiple projects and each project can have multiple versions uploaded, but only the latest one will be used for launching new spiders. Scrapyd-client Scrapyd-client is a client for Scrapyd. It provides the general scrapyd-client and the scrapyd-deploy utility which allows you to deploy your project to a Scrapyd server. 安装 pip install scrapyd scrapyd - client 配置服务器信息 当我们使用scrapy startnewproject来创建新工程时，会自动生成一个scrapy.cfg文件。 # Automatically created by : scrapy startproject # # For more information about the [ deploy ] section see : # https : // scrapyd . readthedocs . org / en / latest / deploy . html [ settings ] default = prototypes . settings [ deploy : proto_server ] url = http : // localhost : 6800 / project = prototypes 其中有两个地方是需要我们更新的： proto_server是自定义的服务器名称 url是指scrapyd启动后，默认的服务器地址 启动scrapyd服务 执行scrapyd，然后用浏览器打开http://localhost:6800/查看界面是否启动成功 部署project 部署有两种方式： 1. 通过json API: addversion.json curl http : // localhost : 6800 / addversion . json - F project = prototypes - F version = r23 - F egg = @prototypes . egg 其中egg需要用scrapyd-deploy来生成 # 进入 scrapy工程目录 scrapyd - deploy --build-egg prototypes.egg 通过scrapyd-deploy一键部署 scrapyd - deploy proto_server - p prototypes 部署好后，我们可以用命令查看 scrapyd - deploy - L proto_server 然后在http://localhost:6800/查看界面发现Available projects： prototypes 运行spider curl http : // localhost : 6800 / schedule . json - d project = prototypes - d spider = qichacha Note: 请务必确认在spider脚本中的初始化中对父类也进行初始化，否则会报错 exceptions.TypeError: __init__() got an unexpected keyword argument '_job' Further [ ] 集成 DormyMo/SpiderKeeper 或 Gerapy/Gerapy","tags":"Development","url":"shi-yong-scrapydhe-scrapyd-clientbu-shu-pa-chong.html"},{"title":"在Centos7上使用Python操作Mysql实践","text":"Centos7上安装mysql sudo yum install mysql mysql - devel # centos7上已经使用mariadb替换了mysql ，它们仅在安装配置上有些区别，使用上没有差别。 sudo yum install mariadb - server 配置mysql # 第一次启动会检查 / var / lib / mysql目录是否为空 ，不为空将返回失败 sudo service mariadb start # 设置 mysql密码 sudo mysql_secure_installation 如果配置时有任何错误，可在/var/log/mariadb/mariadb.log里查看具体原因 Python Mysql库安装 sudo yum install python - devel sudo pip install mysql - python # Toolkit for Python - based database access . sudo pip install dataset dataset常见操作 dataset 库是对SQLAlchemy二次封装，使其操作数据库就像操作json文件一样方便。 连接Mysql db = dataset . connect ( ' mysql://username:password@port/dbname?charset=utf8 ' ) 添加记录 data = { 'key' : 1 , 'column2' : 2 , 'column3' : 3 , } # 插入记录 ， 如果key ， 存在则报错 db [ t_name ] . insert ( data , [ 'key' ] ) # 插入记录 ， 如果key ， 存在则更新 db [ t_name ] . upsert ( data , [ 'key' ] ) 查找记录 # 打印表的字段 print self . db [ ' t_name ' ]. columns # 打印表中所有数据 for row_data in self . db [ ' t_name ' ]. all () : print row_data # 根据多个条件找记录, 返回的是列表 self . db [ ' t_name ' ]. find ( key1 = 1 , key2 = 2 ) # 返回一条记录 self . db [ ' t_name ' ]. find_one ( key1 = 1 , key2 = 2 ) 考虑到find()查找性能，也可以使用原生的sql语句来进行query select * from t_name limit 99 更新记录 data = { 'key' : 1 , 'column2' : 2 , 'column3' : 3 , } self . db [ 't_name' ]. update ( data , [ 'key' ]) 删除记录 self . db [ 't_name' ]. delete ( key1 = 1 , key2 = 2 ) # 删除所有记录 self . db [ 't_name' ]. delete () 关于dataset导出数据 现在dataset被分离成两个模块，如果想导出数据请看 datafreeze 详细用法请见 dataset readthedocs","tags":"Development","url":"zai-centos7shang-shi-yong-pythoncao-zuo-mysqlshi-jian.html"},{"title":"如何在unittest cases之间传递变量","text":"想必大家在写Python UT的时候，偶尔会遇到一些测试用例需要同时测试CRUD的接口。 这时需要在test cases之间传递变量该如何做呢？ 一开始，我们想既然是在一个class里，直接给instance增加attribute不就好了么? import unittest class TestA ( unittest . TestCase ): def __init__ ( self , testname ): super ( TestA , self ) . __init__ ( testname ) self . transmit_var = None def test_1 ( self ): self . transmit_var = 'hello world' def test_2 ( self ): print self . transmit_var if __name__ == \"__main__\" : unittest . main ( verbosity = 2 ) 结果...想当然了。 test_1 ( __main__ . TestA ) ... ok test_2 ( __main__ . TestA ) ... None ok ---------------------------------------------------------------------- Ran 2 tests in 0 . 000 s OK 知识点一：在unittest中, 执行每个testcase的时候，会刷新instance attribute 既然self不行，那cls呢？ 那我们再想想unittest在执行class级别的setup，teardown的时候，其中的class attribute是不是对每个testcase都生效。 通过查看源码可以知道self. class is cls。 import unittest class TestA ( unittest . TestCase ): def test_1 ( self ): self . __class__ . transmit_var = 'hello world' def test_2 ( self ): print self . __class__ . transmit_var if __name__ == \"__main__\" : unittest . main ( verbosity = 2 ) Bingo！ test_1 ( __main__ . TestA ) ... ok test_2 ( __main__ . TestA ) ... hello world ok ---------------------------------------------------------------------- Ran 2 tests in 0 . 000 s OK 知识点二：在unittest中，绑定在class的attribute可以在testcase之间传递 温故而知新： Class and Instance Attributes","tags":"Development","url":"ru-he-zai-unittest-caseszhi-jian-chuan-di-bian-liang.html"},{"title":"BloomFilter For Scrapy Redis","text":"Summary: This article will illustrate how to renovate scrapy-redis to dupefilter. Why use bloomfilter Tips on optimizing scrapy for a high performance How to integrate bloomfilter into scrapy redis copy scrapy_redis into the path alongside settings in scrapy project implement bloom_filter.py in scrapy_redis path the code in https://github.com/acefei/scrapy-redis-docker/blob/master/scrapy_redis_demo/scrapy_redis_demo/bloom_filter.py modify scrapy_redis/dupefilter.py def __init__ ( self , server , key , debug = False ): \"\"\"Initialize the duplicates filter. Parameters ---------- server : redis.StrictRedis The redis server instance. key : str Redis key Where to store fingerprints. debug : bool, optional Whether to log filtered requests. \"\"\" self . server = server self . key = key self . debug = debug self . logdupes = True # 集成布隆过滤器 self . bf = BloomFilter ( conn = server , key = key ) # 利用连接池连接 Redis def request_seen ( self , request ) : \"\"\" Returns True if request was already seen. Parameters ---------- request : scrapy . http . Request Returns ------- bool \"\"\" # 集成布隆过滤器 if self . bf . is_exist ( fp ) : # 判断如果域名在 Redis 里存在 return True else : self . bf . add ( fp ) # 如果不存在，将域名添加到 Redis return False # fp = self . request_fingerprint ( request ) # This returns the number of values added , zero if already exists . # added = self . server . sadd ( self . key , fp ) # return added == 0 Add scrapy_redis configration into settings.py Note: use our scrapy_redis code like .scrapy_redis instead of scrapy_redis that created by pip install ################################## # Configuration for scrapy - redis { DUPEFILTER_CLASS = \" scrapy_redis_demo.scrapy_redis.dupefilter.RFPDupeFilter \" SCHEDULER = \" scrapy_redis_demo.scrapy_redis.scheduler.Scheduler \" SCHEDULER_PERSIST = True ITEM_PIPELINES = { ' scrapy_redis_demo.scrapy_redis.pipelines.RedisPipeline ' : 400 , } # if u add ' network_mode: \"host\" ' in scraper service in docker - compose . yaml # use host ip to access redis server REDIS_HOST = ' 172.16.100.62 ' # else use redis hostname to access redis server # REDIS_HOST = ' redis ' REDIS_PORT = 6379 # Specify your redis uri # the uri scheme syntax : http : // www . iana . org / assignments / uri - schemes / prov / redis # REDIS_URL = ' redis://172.16.100.62:6379 ' # } ################################## import .scrapy_redis in your spiders/xxxxx.py from scrapy_redis_demo.scrapy_redis.spiders import RedisCrawlSpider","tags":"Development","url":"bloomfilter-for-scrapy-redis.html"},{"title":"Web Scraping Practice","text":"Summary Recently, I found a TOEIC test website with enomous mp3 materials. Just in time, I decided to scrape and download MP3 in parallel by python Requirement pip install lxml # https : // github . com / kennethreitz / grequests pip install grequests HTML Scraping lxml is a pretty extensive library written for parsing XML and HTML documents very quickly, even handling messed up tags in the process. We will also be using the Requests module instead of the already built-in urllib2 module due to improvements in speed and readability. About the details, please refer to the link Concurrency We will use GRequests which allows you to use Requests with Gevent to make asynchronous HTTP Requests easily. Or use the fork: FRequests The following need to be noted: 1. Using pool to limit concurrency. GRequests doesn't use pool by default, please see below code snippet: def map ( requests , stream = False , size = None , exception_handler = None , gtimeout = None ) : \"\"\" Concurrently converts a list of Requests to Responses. :param requests : a collection of Request objects . : param stream : If True , the content will not be downloaded immediately . : param size : Specifies the number of requests to make at a time . If None , no throttling occurs . : param exception_handler : Callback function , called when exception occured . Params : Request , Exception : param gtimeout : Gevent joinall timeout in seconds . ( Note : unrelated to requests timeout ) \"\"\" requests = list ( requests ) pool = Pool ( size ) if size else None ... In my requirement, there are enomous links need to settle. If don't constrain concurrency, it would lead to the exception \"gevent.hub.LoopExit: This operation would block forever\". 2. Using session to avoid the fd consuming. There is a same problem with me. Finally, the source code is available in the gist SEE ALSO: http://xlambda.com/gevent-tutorial/","tags":"Development","url":"web-scraping-practice.html"},{"title":"Make a Github Pages blog with Pelican","text":"Make a Github Pages blog with Pelican: Install dependence sudo yum install - y git sudo pip install pelican markdown ghp - import BeautifulSoup4 Create user pages There are two basic types of GitHub Pages: User/Organization Pages and Project Pages . Generally, most people will select User Pages, and there are two caveat as below: You must use the username.github.io naming scheme. Content from the master branch will be used to build and publish your GitHub Pages site. When User Pages are built, they are available at http(s)://username.github.io. Set up the blog with Pelican Create a new branch (pelican) for hosting pelican settings on github, please refer to Publish $ git clone https : // github . com / acefei / acefei . github . io $ cd acefei . github . io / $ git checkout - b pelican $ pelican - quickstart Welcome to pelican - quickstart v3 . 7 . 1 . This script will help you create a new Pelican - based website . Please answer the following questions so this script can generate the files needed by Pelican . > Where do you want to create your new web site ? [.] > What will be the title of this web site ? Acefei ' s Blog > Who will be the author of this web site ? acefei > What will be the default language of this web site ? [ en ] > Do you want to specify a URL prefix ? e . g ., http : // example . com ( Y / n ) > What is your URL prefix ? ( see above example ; no trailing slash) https://acefei.github.io > Do you want to enable article pagination ? ( Y / n ) n > What is your time zone ? [ Europe / Paris ] Asia / Shanghai > Do you want to generate a Fabfile / Makefile to automate generation and publishing ? ( Y / n ) > Do you want an auto - reload & simpleHTTP script to assist with theme and site development ? ( Y / n ) > Do you want to upload your website using FTP ? ( y / N ) > Do you want to upload your website using SSH ? ( y / N ) > Do you want to upload your website using Dropbox ? ( y / N ) > Do you want to upload your website using S3 ? ( y / N ) > Do you want to upload your website using Rackspace Cloud Files ? ( y / N ) > Do you want to upload your website using GitHub Pages ? ( y / N ) Y > Is this your personal page ( username . github . io ) ? ( y / N ) Y Write first post To facilitate blog creation, I write a script for creating the template with md format. Generate HTML pages and pre-view via http://localhost:8000/ make html && make serve & firefox http : // localhost : 8000 / # After pre - view fg # Then , Ctrl + C to terminate the process Publish If everything is OK, generate the website. Currently, all pelican settings that are used to render HTML are on pelican branch. As previously mentioned, the static website content should be pulish from master branch. So, I need to publish respectively: For static website: (on master branch ) make github For pelican settings: (on pelican branch ) echo - e \"*.pyc\\noutput/\" >> . gitignore git add . git commit - m \"commit pelican setting\" make pelican Thanks to github action , now you just to edit your article and commit on pelican branch, it will deploy automatically. (See ci.yaml ) Extension Theme Clone your fevorite theme , such as elegant mkdir pelican - themes cd pelican - themes git clone git : // github . com / talha131 / pelican - elegant . git Then, add something like this to pelicanconf.py THEME = \"pelican-themes/pelican-elegant\" Note: 1. Under GFW, we need to find an alternative CDN site to replace googleapis in theme folder. 2. If you want to add the theme into your pelican branch, remove the .git* path under the theme folder. Plugin Clone the plugin repo. git clone git : // github . com / getpelican / pelican - plugins . git Then, add following contents into pelicanconf.py ###### plugin configuration ####### PLUGIN_PATHS = [ 'pelican-plugins' ] PLUGINS = [ 'sitemap' , 'extract_toc' , 'tipue_search' ] SITEMAP = { 'format' : 'xml' , 'priorities' : { 'articles' : 0 . 5 , 'indexes' : 0 . 5 , 'pages' : 0 . 5 } , 'changefreqs' : { 'articles' : 'weekly' , 'indexes' : 'daily' , 'pages' : 'monthly' } } Ok, plugin install completely. Pelican settings There are enhancements in pelicanconfig.py Reference Blog onCrash=Reboot(); uses Elegant theme. You can see its configuration files at Github for inspiration.","tags":"Development","url":"make-a-github-pages-blog-with-pelican.html"}]}